{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import senseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size=3):\n",
    "        self.window_size = window_size\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.porter_to_wn = {\n",
    "            \"NOUN\": wn.NOUN,\n",
    "            \"VERB\": wn.VERB,\n",
    "            \"ADJ\" : wn.ADJ\n",
    "        }\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        self.rel_pairs = {\n",
    "            \"NOUN\": [\n",
    "                (\"gloss\", \"gloss\"), (\"hypo\", \"hypo\"), (\"mero\", \"mero\"),\n",
    "                (\"gloss\", \"hypo\"), (\"gloss\", \"mero\"), (\"hypo\", \"mero\"),\n",
    "                (\"hypo\", \"gloss\"), (\"mero\", \"gloss\"), (\"mero\", \"hypo\")\n",
    "            ],\n",
    "            \"ADJ\" : [\n",
    "                (\"gloss\", \"gloss\"), (\"anto\", \"anto\"), (\"sim\", \"sim\"),\n",
    "                (\"gloss\", \"anto\"), (\"gloss\", \"sim\"), (\"anto\", \"sim\"),\n",
    "                (\"anto\", \"gloss\"), (\"sim\", \"gloss\"), (\"sim\", \"anto\")\n",
    "            ], \n",
    "            \"VERB\": [\n",
    "                (\"gloss\", \"gloss\"), (\"entl\", \"entl\"),\n",
    "                (\"gloss\", \"entl\"), (\"entl\", \"gloss\")\n",
    "            ],\n",
    "            \"default\": [(\"gloss\", \"gloss\")]\n",
    "        }\n",
    "        \n",
    "        \n",
    "    # a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "    # b = [\"b\", \"c\", \"x\", \"c\", \"d\"]\n",
    "    # disambiguator = Disambiguator()\n",
    "    # length, new_a, new_b = disambiguator.get_longest_common_substring(a, b)\n",
    "    # print(length) # 2\n",
    "    # print(new_a)  # [\"a\", \"*\", \"*\", \"d\"]\n",
    "    # print(new_b)  # [\"*\", \"*\", \"x\", \"c\", \"d\"]\n",
    "    def get_longest_common_substring(self, tokens_a, tokens_b):\n",
    "        lcs = [[0] * (1 + len(tokens_b)) for i in range(1 + len(tokens_a))]\n",
    "        best, best_position_a, best_position_b = 0, 0, 0\n",
    "        for i in range(1, 1 + len(tokens_a)):\n",
    "            for j in range(1, 1 + len(tokens_b)):\n",
    "                if (tokens_a[i - 1] == tokens_b[j - 1]) & (tokens_a[i-1] != \"*\"):\n",
    "                    lcs[i][j] = lcs[i - 1][j - 1] + 1\n",
    "                    if lcs[i][j] > best:\n",
    "                        best = lcs[i][j]\n",
    "                        best_position_a = i\n",
    "                        best_position_b = j\n",
    "                else:\n",
    "                    lcs[i][j] = 0\n",
    "                    \n",
    "        tokens_a[best_position_a - best:best_position_a] = \"*\" * best\n",
    "        tokens_b[best_position_b - best:best_position_b] = \"*\" * best\n",
    "        \n",
    "        return best, tokens_a, tokens_b\n",
    "    \n",
    "    # a = \"a b c d\"\n",
    "    # b = \"b c x c d\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # overlap_score = disambiguator.get_overlap_score(a, b)\n",
    "    # print(overlap_score) # 5 = 4 + 1 = 2^2 + 1^2\n",
    "    def get_overlap_score(self, text_a, text_b):\n",
    "        overlap_score = 0\n",
    "        tokens_a = nltk.word_tokenize(text_a)\n",
    "        tokens_b = nltk.word_tokenize(text_b)\n",
    "        sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        while sequence_length > 0:\n",
    "            overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "            sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        \n",
    "        return overlap_score\n",
    "    \n",
    "    def get_texts(self, target_tuple, window_tuple):\n",
    "        target = {}\n",
    "        window = {}\n",
    "        \n",
    "        target[\"gloss\"] = self.get_gloss_for_sense(target_tuple[0])\n",
    "        window[\"gloss\"] = self.get_gloss_for_sense(window_tuple[0])\n",
    "        target[\"hypo\"] = self.get_hyponyms_for_sense(target_tuple[0])\n",
    "        window[\"hypo\"] = self.get_hyponyms_for_sense(window_tuple[0])\n",
    "        target[\"mero\"] = self.get_meronyms_for_sense(target_tuple[0])\n",
    "        window[\"mero\"] = self.get_meronyms_for_sense(window_tuple[0])\n",
    "        target[\"anto\"] = self.get_antonyms_for_sense(target_tuple[0])\n",
    "        window[\"anto\"] = self.get_antonyms_for_sense(window_tuple[0])\n",
    "        target[\"sim\"] = self.get_similarity_for_sense(target_tuple[0])\n",
    "        window[\"sim\"] = self.get_similarity_for_sense(window_tuple[0])\n",
    "        target[\"entl\"] = self.get_entailments_for_sense(target_tuple[0])\n",
    "        window[\"entl\"] = self.get_entailments_for_sense(window_tuple[0])\n",
    "        \n",
    "        return target, window\n",
    "    \n",
    "    def get_enhanced_relatedness(self, target_pos, target_texts, window_texts):\n",
    "        relatedness = 0\n",
    "        for rel_pair in self.rel_pairs[target_pos]:\n",
    "            relatedness = relatedness + self.get_overlap_score(target_texts[rel_pair[0]], window_texts[rel_pair[1]])\n",
    "        return relatedness\n",
    "    \n",
    "    \n",
    "    # mass =  wn.synsets(\"mass\", wn.NOUN)[0]\n",
    "    # print(mass)\n",
    "    # kilogram = wn.synsets(\"kilogram\", wn.NOUN)[0]\n",
    "    # print(kilogram)\n",
    "    # target_tuple, window_tuple =(mass, 'NOUN'), (kilogram, 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # relatedness_score = disambiguator.get_relatedness(target_tuple, window_tuple)\n",
    "    # print(relatedness_score)\n",
    "    def get_relatedness(self, target_tuple, window_tuple):\n",
    "        target_texts, window_texts = self.get_texts(target_tuple, window_tuple)\n",
    "        if target_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "            relatedness = self.get_enhanced_relatedness(target_tuple[1], target_texts, window_texts)\n",
    "        else:\n",
    "            relatedness = self.get_overlap_score(\"default\", target_texts, window_texts)\n",
    "            \n",
    "        return relatedness\n",
    "    \n",
    "    def get_gloss_for_sense(self, sense):\n",
    "        gloss_for_sense = \"\"\n",
    "        gloss_for_sense = gloss_for_sense + sense.definition()\n",
    "        for example in sense.examples():\n",
    "            gloss_for_sense =gloss_for_sense + \". \" + example\n",
    "        return gloss_for_sense\n",
    "    \n",
    "    def merge_glosses(self, synsets):\n",
    "        aggregator = \"\"\n",
    "        for synset in synsets:\n",
    "            aggregator = aggregator + \". \" + self.get_gloss_for_sense(synset)\n",
    "        return aggregator\n",
    "    \n",
    "    def get_hyponyms_for_sense(self, sense):\n",
    "        hyponyms_for_sense = self.merge_glosses(sense.hyponyms())\n",
    "        return hyponyms_for_sense\n",
    "    \n",
    "    def get_meronyms_for_sense(self, sense):\n",
    "        meronyms_for_sense = self.merge_glosses(sense.member_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.substance_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.part_meronyms())\n",
    "        return meronyms_for_sense\n",
    "    \n",
    "    def get_antonyms_for_sense(self, sense):\n",
    "        antonyms_for_sense = \"\"\n",
    "        for lemma in sense.lemmas():\n",
    "            antonyms_for_sense = antonyms_for_sense + \". \" + self.merge_glosses([ant.synset() for ant in lemma.antonyms()])\n",
    "        return antonyms_for_sense\n",
    "    \n",
    "    def get_similarity_for_sense(self, sense):\n",
    "        similarity_for_sense = self.merge_glosses(sense.similar_tos())\n",
    "        return similarity_for_sense\n",
    "    \n",
    "    def get_entailments_for_sense(self, sense):\n",
    "        entailments_for_sense = self.merge_glosses(sense.entailments())\n",
    "        return entailments_for_sense\n",
    "    \n",
    "    def get_target_sense_score(self, target_sense_tuple):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        for window_word in self.window_words:\n",
    "            #print(window_word)\n",
    "            window_word_senses = self.get_tuple_senses(window_word)\n",
    "            for window_word_sense in window_word_senses:\n",
    "                relatedness = self.get_relatedness(target_sense_tuple, (window_word_sense, window_word[1]))\n",
    "                target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "            if word_tuple[0] not in stopwords.words('english'): \n",
    "                if word_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "                    filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def remove_punctuation(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text:  \n",
    "            if (len(word_tuple) == 2):\n",
    "                if (word_tuple[1] != \".\") : # word_tuple[0] not in [\".\", \"`\"]: \n",
    "                    filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return processed_text[i], i\n",
    "        return None, -1\n",
    "    \n",
    "    # word_tuple = ('mass', 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # tuple_senses = disambiguator.get_tuple_senses(word_tuple)\n",
    "    # print(tuple_senses)\n",
    "    # # [Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n",
    "    def get_tuple_senses(self, word_tuple):\n",
    "        return wn.synsets(word_tuple[0], self.porter_to_wn[word_tuple[1]])\n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    # processed_text = [('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    # stemmed_target = \"mass\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # target_tuple, window_words = disambiguator.get_window_words(processed_text, stemmed_target)\n",
    "    # print(target_tuple)\n",
    "    # print(window_words)\n",
    "    # # ('mass', 'NOUN')\n",
    "    # # [('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    def get_window_words(self, processed_text, stemmed_target):\n",
    "        processed_text = self.remove_punctuation(processed_text)\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_tuple, target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        if target_position == -1:\n",
    "            print(\"Target word is not in the text!\")\n",
    "            return None, -1\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = processed_text[0:target_position] + processed_text[target_position+1:]\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            left_words = processed_text[0:target_position]\n",
    "            right_words = processed_text[target_position+1:2 * self.window_size + 1]\n",
    "            # target_position + self.window_size + 1 + (self.window_size - target_position) =  2 * self.window_size + 1\n",
    "            # print(\"Extract everything from the left and (self.window_size - target_position) more from the right\")\n",
    "            window_words=left_words+right_words\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            right_words = processed_text[target_position+1:]\n",
    "            left_words = processed_text[len(processed_text) - 2*self.window_size - 1:target_position]\n",
    "            # target_position-self.window_size-(self.window_size - (len(processed_text) - target_position - 1)) =\n",
    "            # = -2*self.window_size + target_position - target_position + len(processed_text) - 1 = \n",
    "            # = len(processed_text) - 2*self.window_size - 1\n",
    "            # print(\"Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\")\n",
    "            window_words=left_words+right_words\n",
    "        else:\n",
    "            left_words = processed_text[target_position-self.window_size:target_position]\n",
    "            right_words = processed_text[target_position+1:target_position+1+self.window_size]\n",
    "            # print(\"Extract self.window_size word tuples from each side\")\n",
    "            window_words=left_words+right_words\n",
    "        return target_tuple, window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.get_processed(\"the mass of the object is ten kilograms\")\n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "#         print(\"Tokens with punctuation\")\n",
    "#         print(tokenized_words_with_punctuation)\n",
    "#         tokenized_words = self.remove_punctuation(tokenized_words_with_punctuation)\n",
    "#         print(\"Tokens\")\n",
    "#         print(tokenized_words)\n",
    "        word_pos_pairs = nltk.pos_tag(tokenized_words, tagset='universal')\n",
    "#         print(\"POS\")\n",
    "#         print(word_pos_pairs)\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "#         print(\"Stemmed\")\n",
    "#         print(stemmed_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    # eg:\n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "#         print(processed_text)\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        \n",
    "#         print(target_word)\n",
    "#         print(stemmed_target)\n",
    "        target_tuple, self.window_words = self.get_window_words(processed_text, stemmed_target)\n",
    "        if target_tuple == None:\n",
    "            return\n",
    "#         print(target_tuple)\n",
    "        target_senses = self.get_tuple_senses((target_word, target_tuple[1]))\n",
    "        best_score = (None, -1)\n",
    "#         print(target_senses)\n",
    "        for target_sense in target_senses:\n",
    "            target_sense_score = self.get_target_sense_score((target_sense, target_tuple[1]))\n",
    "            if target_sense_score > best_score[1] :\n",
    "                best_score = (target_sense, target_sense_score)\n",
    "        \n",
    "        predicted_synset = best_score[0]\n",
    "        disambiguated_sense = self.get_gloss_for_sense(best_score[0])\n",
    "        return predicted_synset, disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DisambiguatorTester:\n",
    "    def __init__(self, window_size=3):\n",
    "        self.disambiguator = Disambiguator(window_size)\n",
    "        self.sense_map = {\n",
    "            \"HARD1\": [\"difficult.a.01\"],    # not easy, requiring great physical or mental\n",
    "            \"HARD2\": [\"hard.a.02\",          # dispassionate\n",
    "                      \"difficult.a.01\"],\n",
    "            \"HARD3\": [\"hard.a.03\"],         # resisting weight or pressure\n",
    "            \"interest_1\": [\"interest.n.01\"], # readiness to give attention\n",
    "            \"interest_2\": [\"interest.n.03\"], # quality of causing attention to be given to\n",
    "            \"interest_3\": [\"pastime.n.01\"],  # activity, etc. that one gives attention to\n",
    "            \"interest_4\": [\"sake.n.01\"],     # advantage, advancement or favor\n",
    "            \"interest_5\": [\"interest.n.05\"], # a share in a company or business\n",
    "            \"interest_6\": [\"interest.n.04\"], # money paid for the use of money\n",
    "            \"cord\": [\"line.n.18\"],          # something (as a cord or rope) that is long and thin and flexible\n",
    "            \"formation\": [\"line.n.01\",\"line.n.03\"], # a formation of people or things one beside another\n",
    "            \"text\": [\"line.n.05\"],                 # text consisting of a row of words written across a page or computer screen\n",
    "            \"phone\": [\"telephone_line.n.02\"],   # a telephone connection\n",
    "            \"product\": [\"line.n.22\"],       # a particular kind of product or merchandise\n",
    "            \"division\": [\"line.n.29\"],      # a conceptual separation or distinction\n",
    "            \"SERVE12\": [\"serve.v.02\"],       # do duty or hold offices; serve in a specific function\n",
    "            \"SERVE10\": [\"serve.v.06\"], # provide (usually but not necessarily food)\n",
    "            \"SERVE2\": [\"serve.v.01\"],       # serve a purpose, role, or function\n",
    "            \"SERVE6\": [\"service.v.01\"]      # be used by; as of a utility\n",
    "        }\n",
    "        \n",
    "    def get_test_case(self, instance):\n",
    "        pos = instance.position\n",
    "        target_word = ' '.join(w for (w,t) in instance.context[pos:pos+1])\n",
    "        \n",
    "#         print(instance.context[0:pos])\n",
    "        left = ' '.join( \"\" if word_tuple=='FRASL' else word_tuple[0] for word_tuple in instance.context[0:pos]) # .items()\n",
    "        right = ' '.join(w for (w,t) in instance.context[pos+1:])\n",
    "        phrase = left + \" \" + target_word + \" \" + right\n",
    "        \n",
    "        target_synsets_names = self.sense_map[instance.senses[0]]\n",
    "        target_synsets = [wn.synset(name) for name in target_synsets_names]\n",
    "        \n",
    "        return phrase, target_word, target_synsets\n",
    "        \n",
    "    def test(self):\n",
    "        correct_predictions, total_tests = 0, 0\n",
    "        corpuses = senseval.fileids()\n",
    "        for corpus in corpuses[2:]:\n",
    "            print(\"=\" * 100)\n",
    "#             no_instances = len(senseval.instances(corpus))\n",
    "#             print(\"Testing \" + corpus + \" with \" + str(no_instances) + \" instances.\" )\n",
    "            print(\"Testing \" + corpus)\n",
    "            print(\"=\" * 100)\n",
    "            for instance in senseval.instances(corpus)[0:2]:\n",
    "                phrase, target_word, target_synsets = self.get_test_case(instance)\n",
    "                print(\"-\" * 100)\n",
    "                print(\"Phrase: \" + phrase)\n",
    "                print(\"_\" * 5)\n",
    "                print(\"Target Word: \" + target_word)\n",
    "                print(\"_\" * 5)\n",
    "                print(\"Target Synsets: \" + str(target_synsets) + \" meaning: \")\n",
    "                print(target_synsets[0].definition())\n",
    "                print(\"_\" * 5)\n",
    "                predicted_synset, description = self.disambiguator.disambiguate(phrase, target_word)\n",
    "                print(\"Predicted Synset: \" + str(predicted_synset) + \" meaning: \")\n",
    "                print(description)\n",
    "                if predicted_synset in target_synsets:\n",
    "                    correct_predictions += 1\n",
    "                total_tests +=1\n",
    "        print(\"=\" * 100)\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Run \" + str(total_tests) + \" tests\")\n",
    "        print(\"Predicted \" + str(correct_predictions) + \" predictions\")\n",
    "        print(\"Percentage \" + str(correct_predictions / total_tests * 100) + \"%\")\n",
    "        print(\"=\" * 100)\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tester = DisambiguatorTester(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
