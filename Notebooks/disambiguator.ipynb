{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"the mass of the object is ten kilograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"the mass of the object is ten kilograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(text, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syns = wn.synsets('dying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('grave.n.01')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('death.n.04.death'),\n",
       " Lemma('death.n.04.dying'),\n",
       " Lemma('death.n.04.demise')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('birth.n.01.birth')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kilograms'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed = (stemmer.stem(pos[7][0]), pos[7][1])\n",
    "stemmed_pairs.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kilogram', 'NOUN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"dying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.porter_to_wn = {\n",
    "            \"NOUN\": wn.NOUN,\n",
    "            \"VERB\": wn.VERB,\n",
    "            \"ADJ\" : wn.ADJ\n",
    "        }\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        \n",
    "    def get_longest_common_subsequence(text_a, text_b):\n",
    "        # !IMPORTANT, after finding the sequence, mark the words from both text_a and text_b\n",
    "        # We do this to avoid using a word in multiple sequences\n",
    "        # eg:\n",
    "        # a b c d\n",
    "        # b c x c d \n",
    "        # the c in the first string will be part of the bc sequence but not of cd\n",
    "        return sequence_length, text_a, text_b\n",
    "        \n",
    "    def get_overlap_score(text_a, text_b):\n",
    "        overlap_score = 0\n",
    "        # while length > 0\n",
    "            # sequence_length, text_a, text_b = get_longest_common_subsequence(text_a, text_b)\n",
    "            # overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "        \n",
    "        return overlap_score\n",
    "    \n",
    "    def get_relatedness(sense_a, sense_b):\n",
    "        relatedness = 0\n",
    "        relatedness= relatedness + self.get_overlap_score(self.get_text_for_sense(sense_a), self.get_text_for_sense(sense_b))\n",
    "        return relatedness\n",
    "    \n",
    "    def get_text_for_sense(self, sense):\n",
    "        text_for_sense = \"\"\n",
    "        text_for_sense = text_for_sense + sense.definition()\n",
    "        for example in sense.examples():\n",
    "            text_for_sense = text_for_sense + \". \" + example\n",
    "        return text_for_sense\n",
    "        \n",
    "    def get_target_sense_score(self, target_sense):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        for window_word in self.window_words:\n",
    "            window_word_senses = self.get_tuple_senses(window_word)\n",
    "            for window_word_sense in window_word_senses:\n",
    "                relatedness = self.get_relatedness(target_sense, window_word_sense)\n",
    "                target_sense_score = target_sense_score + relatedness\n",
    "        # for each window_word in self.window_words\n",
    "            # get all window_word_senses for the window_word\n",
    "            # for each window_word_sense in window_word_senses\n",
    "                # relatedness = get_relatedness(target_sense, window_word_sense)\n",
    "                # target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "#     def get_window_words(self, text_string):\n",
    "#         print(\"Set self.window_words here\")\n",
    "        # do POS-tagging\n",
    "        # remove words without meaning( the)\n",
    "        # get self.window_size words from the left and from the right\n",
    "            # if not enough words on one side, \n",
    "                # add the rest from the opposite side\n",
    "            # if not enough words on either sides(i.e. len(words(text_string)) < 2 * self.window_size + 1 ) \n",
    "                # just use all words in text_string except the target_word\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "          if word_tuple[0] not in stopwords.words('english'): \n",
    "            filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return processed_text[i], i\n",
    "        return None, -1\n",
    "    \n",
    "    def get_tuple_senses(self, word_tuple):\n",
    "        return wn.synsets(word_tuple[0], self.porter_to_wn[word_tuple[1]])\n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    \n",
    "    def get_window_words(self, processed_text, stemmed_target):\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_tuple, target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        print(\"Target position is \" + str(target_position))\n",
    "        if target_position == -1:\n",
    "            print(\"Target word is not in the text!\")\n",
    "            return None, -1\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = processed_text[0:target_position] + processed_text[target_position+1:]\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            left_words = processed_text[0:target_position]\n",
    "            right_words = processed_text[target_position+1:2 * self.window_size + 1]\n",
    "            # target_position + self.window_size + 1 + (self.window_size - target_position) =  2 * self.window_size + 1\n",
    "            print(\"Extract everything from the left and (self.window_size - target_position) more from the right\")\n",
    "            window_words=left_words+right_words\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            right_words = processed_text[target_position+1:]\n",
    "            left_words = processed_text[len(processed_text) - 2*self.window_size - 1:target]\n",
    "            # target_position-self.window_size-(self.window_size - (len(processed_text) - target_position - 1)) =\n",
    "            # = -2*self.window_size + target_position - target_position + len(processed_text) - 1 = \n",
    "            # = len(processed_text) - 2*self.window_size - 1\n",
    "            print(\"Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\")\n",
    "            window_words=left_words+right_words\n",
    "        else:\n",
    "            left_words = processed_text[target_position-self.window_size:target_position]\n",
    "            right_words = processed_text[target_position+1:target_position+1+self.window_size]\n",
    "            print(\"Extract self.window_size word tuples from each side\")\n",
    "            window_words=left_words+right_words\n",
    "        return target_tuple, window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "        word_pos_pairs = nltk.pos_tag(text, tagset='universal')\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    \n",
    "    # eg:\n",
    "    # > disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # > disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "        print(processed_text)\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        target_tuple, self.window_words = self.get_window_words(processed_text, stemmed_target)\n",
    "        if target_tuple == None:\n",
    "            return\n",
    "        print(self.window_words)\n",
    "        target_senses = self.get_tuple_senses(target_tuple)\n",
    "        print(target_senses)\n",
    "        best_sense = (None, 0)\n",
    "        for target_sense in target_senses:\n",
    "            print(self.get_text_for_sense(target_sense))\n",
    "        # for each target_sense in target_senses\n",
    "            # calculate the target_sense_score\n",
    "            target_sense_score = self.get_target_sense_score(target_sense)\n",
    "            if target_sense_score > best_score[1] :\n",
    "                best_score = (target_sense, target_sense_score)\n",
    "        # take the target_sense with the \n",
    "        disambiguated_sense = self.get_text_for_sense(best_score[0])\n",
    "        return disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disambiguator = Disambiguator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "Target position is 1\n",
      "[('mass', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "[Synset('object.n.01'), Synset('aim.n.02'), Synset('object.n.03'), Synset('object.n.04'), Synset('object.n.05')]\n",
      "a tangible and visible entity; an entity that can cast a shadow. it was full of rackets, balls and other objects\n",
      "the goal intended to be attained (and which is believed to be attainable). the sole object of her trip was to see her children\n",
      "(grammar) a constituent that is acted upon. the object of the verb\n",
      "the focus of cognitions or feelings. objects of thought. the object of my affection\n",
      "(computing) a discrete item that provides a description of virtually anything known to a computer. in object-oriented programming, objects include data and define its status, its methods of operation and how it interacts with other objects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Implemented Yet'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'Synset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-adad6ecf0a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mass.n.01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'Synset'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
