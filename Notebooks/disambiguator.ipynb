{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"the mass of the object is ten kilograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"the mass of the object is ten kilograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(text, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syns = wn.synsets('dying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass = wn.synsets('car')\n",
    "mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Lemma' object has no attribute 'meronyms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-db3e1091dd75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeronyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Lemma' object has no attribute 'meronyms'"
     ]
    }
   ],
   "source": [
    "mass[0].lemmas()[0].meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass[0].member_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass[0].substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('accelerator.n.01'),\n",
       " Synset('air_bag.n.01'),\n",
       " Synset('auto_accessory.n.01'),\n",
       " Synset('automobile_engine.n.01'),\n",
       " Synset('automobile_horn.n.01'),\n",
       " Synset('buffer.n.06'),\n",
       " Synset('bumper.n.02'),\n",
       " Synset('car_door.n.01'),\n",
       " Synset('car_mirror.n.01'),\n",
       " Synset('car_seat.n.01'),\n",
       " Synset('car_window.n.01'),\n",
       " Synset('fender.n.01'),\n",
       " Synset('first_gear.n.01'),\n",
       " Synset('floorboard.n.02'),\n",
       " Synset('gasoline_engine.n.01'),\n",
       " Synset('glove_compartment.n.01'),\n",
       " Synset('grille.n.02'),\n",
       " Synset('high_gear.n.01'),\n",
       " Synset('hood.n.09'),\n",
       " Synset('luggage_compartment.n.01'),\n",
       " Synset('rear_window.n.01'),\n",
       " Synset('reverse.n.02'),\n",
       " Synset('roof.n.02'),\n",
       " Synset('running_board.n.01'),\n",
       " Synset('stabilizer_bar.n.01'),\n",
       " Synset('sunroof.n.01'),\n",
       " Synset('tail_fin.n.02'),\n",
       " Synset('third_gear.n.01'),\n",
       " Synset('window.n.02')]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass[0].part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass[0].substance_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('death.n.04.death'),\n",
       " Lemma('death.n.04.dying'),\n",
       " Lemma('death.n.04.demise')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('birth.n.01.birth')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bad.a.01'),\n",
       " Synset('bad.s.02'),\n",
       " Synset('bad.s.03'),\n",
       " Synset('bad.s.04'),\n",
       " Synset('regretful.a.01'),\n",
       " Synset('bad.s.06'),\n",
       " Synset('bad.s.07'),\n",
       " Synset('bad.s.08'),\n",
       " Synset('bad.s.09'),\n",
       " Synset('bad.s.10'),\n",
       " Synset('bad.s.11'),\n",
       " Synset('bad.s.12'),\n",
       " Synset('bad.s.13'),\n",
       " Synset('bad.s.14')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = wn.synsets('bad', wn.ADJ)\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('atrocious.s.02'),\n",
       " Synset('corked.s.01'),\n",
       " Synset('deplorable.s.01'),\n",
       " Synset('fearful.s.04'),\n",
       " Synset('hard.s.11'),\n",
       " Synset('hopeless.s.04'),\n",
       " Synset('horrid.s.01'),\n",
       " Synset('icky.s.01'),\n",
       " Synset('ill.s.03'),\n",
       " Synset('incompetent.s.04'),\n",
       " Synset('mediocre.s.03'),\n",
       " Synset('naughty.s.02'),\n",
       " Synset('negative.s.03'),\n",
       " Synset('poor.s.06'),\n",
       " Synset('pretty.s.02'),\n",
       " Synset('rubber.s.01'),\n",
       " Synset('severe.s.06'),\n",
       " Synset('swingeing.s.01'),\n",
       " Synset('uncool.s.01'),\n",
       " Synset('unfavorable.s.03'),\n",
       " Synset('unsuitable.s.03')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad[0].similar_tos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kilograms'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed = (stemmer.stem(pos[7][0]), pos[7][1])\n",
    "stemmed_pairs.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kilogram', 'NOUN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"dying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size=3):\n",
    "        self.window_size = window_size\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.porter_to_wn = {\n",
    "            \"NOUN\": wn.NOUN,\n",
    "            \"VERB\": wn.VERB,\n",
    "            \"ADJ\" : wn.ADJ\n",
    "        }\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        self.rel_pairs = {\n",
    "            \"NOUN\": [\n",
    "                (\"gloss\", \"gloss\"), (\"hypo\", \"hypo\"), (\"mero\", \"mero\"),\n",
    "                (\"gloss\", \"hypo\"), (\"gloss\", \"mero\"), (\"hypo\", \"mero\"),\n",
    "                (\"hypo\", \"gloss\"), (\"mero\", \"gloss\"), (\"mero\", \"hypo\")\n",
    "            ],\n",
    "            \"ADJ\" : [\n",
    "                (\"gloss\", \"gloss\"), (\"anto\", \"anto\"), (\"sim\", \"sim\"),\n",
    "                (\"gloss\", \"anto\"), (\"gloss\", \"sim\"), (\"anto\", \"sim\"),\n",
    "                (\"anto\", \"gloss\"), (\"sim\", \"gloss\"), (\"sim\", \"anto\")\n",
    "            ], \n",
    "            \"VERB\": [\n",
    "                (\"gloss\", \"gloss\"), (\"entl\", \"entl\"),\n",
    "                (\"gloss\", \"entl\"), (\"entl\", \"gloss\")\n",
    "            ],\n",
    "            \"default\": [(\"gloss\", \"gloss\")]\n",
    "        }\n",
    "        \n",
    "        \n",
    "    # a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "    # b = [\"b\", \"c\", \"x\", \"c\", \"d\"]\n",
    "    # disambiguator = Disambiguator()\n",
    "    # length, new_a, new_b = disambiguator.get_longest_common_substring(a, b)\n",
    "    # print(length) # 2\n",
    "    # print(new_a)  # [\"a\", \"*\", \"*\", \"d\"]\n",
    "    # print(new_b)  # [\"*\", \"*\", \"x\", \"c\", \"d\"]\n",
    "    def get_longest_common_substring(self, tokens_a, tokens_b):\n",
    "        lcs = [[0] * (1 + len(tokens_b)) for i in range(1 + len(tokens_a))]\n",
    "        best, best_position_a, best_position_b = 0, 0, 0\n",
    "        for i in range(1, 1 + len(tokens_a)):\n",
    "            for j in range(1, 1 + len(tokens_b)):\n",
    "                if (tokens_a[i - 1] == tokens_b[j - 1]) & (tokens_a[i-1] != \"*\"):\n",
    "                    lcs[i][j] = lcs[i - 1][j - 1] + 1\n",
    "                    if lcs[i][j] > best:\n",
    "                        best = lcs[i][j]\n",
    "                        best_position_a = i\n",
    "                        best_position_b = j\n",
    "                else:\n",
    "                    lcs[i][j] = 0\n",
    "                    \n",
    "        tokens_a[best_position_a - best:best_position_a] = \"*\" * best\n",
    "        tokens_b[best_position_b - best:best_position_b] = \"*\" * best\n",
    "        \n",
    "        return best, tokens_a, tokens_b\n",
    "    \n",
    "    # a = \"a b c d\"\n",
    "    # b = \"b c x c d\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # overlap_score = disambiguator.get_overlap_score(a, b)\n",
    "    # print(overlap_score) # 5 = 4 + 1 = 2^2 + 1^2\n",
    "    def get_overlap_score(self, text_a, text_b):\n",
    "        overlap_score = 0\n",
    "        tokens_a = nltk.word_tokenize(text_a)\n",
    "        tokens_b = nltk.word_tokenize(text_b)\n",
    "        sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        while sequence_length > 0:\n",
    "            overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "            sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        \n",
    "        return overlap_score\n",
    "    \n",
    "    def get_texts(self, target_tuple, window_tuple):\n",
    "        target = {}\n",
    "        window = {}\n",
    "        \n",
    "        target[\"gloss\"] = self.get_gloss_for_sense(target_tuple[0])\n",
    "        window[\"gloss\"] = self.get_gloss_for_sense(window_tuple[0])\n",
    "        target[\"hypo\"] = self.get_hyponyms_for_sense(target_tuple[0])\n",
    "        window[\"hypo\"] = self.get_hyponyms_for_sense(window_tuple[0])\n",
    "        target[\"mero\"] = self.get_meronyms_for_sense(target_tuple[0])\n",
    "        window[\"mero\"] = self.get_meronyms_for_sense(window_tuple[0])\n",
    "        target[\"anto\"] = self.get_antonyms_for_sense(target_tuple[0])\n",
    "        window[\"anto\"] = self.get_antonyms_for_sense(window_tuple[0])\n",
    "        target[\"sim\"] = self.get_similarity_for_sense(target_tuple[0])\n",
    "        window[\"sim\"] = self.get_similarity_for_sense(window_tuple[0])\n",
    "        target[\"entl\"] = self.get_entailments_for_sense(target_tuple[0])\n",
    "        window[\"entl\"] = self.get_entailments_for_sense(window_tuple[0])\n",
    "        \n",
    "        return target, window\n",
    "    \n",
    "    def get_enhanced_relatedness(self, target_pos, target_texts, window_texts):\n",
    "        relatedness = 0\n",
    "        for rel_pair in self.rel_pairs[target_pos]:\n",
    "            relatedness = relatedness + self.get_overlap_score(target_texts[rel_pair[0]], window_texts[rel_pair[1]])\n",
    "        return relatedness\n",
    "    \n",
    "    \n",
    "    # mass =  wn.synsets(\"mass\", wn.NOUN)[0]\n",
    "    # print(mass)\n",
    "    # kilogram = wn.synsets(\"kilogram\", wn.NOUN)[0]\n",
    "    # print(kilogram)\n",
    "    # target_tuple, window_tuple =(mass, 'NOUN'), (kilogram, 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # relatedness_score = disambiguator.get_relatedness(target_tuple, window_tuple)\n",
    "    # print(relatedness_score)\n",
    "    def get_relatedness(self, target_tuple, window_tuple):\n",
    "        target_texts, window_texts = self.get_texts(target_tuple, window_tuple)\n",
    "        if target_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "            relatedness = self.get_enhanced_relatedness(target_tuple[1], target_texts, window_texts)\n",
    "        else:\n",
    "            relatedness = self.get_overlap_score(\"default\", target_texts, window_texts)\n",
    "            \n",
    "        return relatedness\n",
    "    \n",
    "    def get_gloss_for_sense(self, sense):\n",
    "        gloss_for_sense = \"\"\n",
    "        gloss_for_sense = gloss_for_sense + sense.definition()\n",
    "        for example in sense.examples():\n",
    "            gloss_for_sense =gloss_for_sense + \". \" + example\n",
    "        return gloss_for_sense\n",
    "    \n",
    "    def merge_glosses(self, synsets):\n",
    "        aggregator = \"\"\n",
    "        for synset in synsets:\n",
    "            aggregator = aggregator + \". \" + self.get_gloss_for_sense(synset)\n",
    "        return aggregator\n",
    "    \n",
    "    def get_hyponyms_for_sense(self, sense):\n",
    "        hyponyms_for_sense = self.merge_glosses(sense.hyponyms())\n",
    "        return hyponyms_for_sense\n",
    "    \n",
    "    def get_meronyms_for_sense(self, sense):\n",
    "        meronyms_for_sense = self.merge_glosses(sense.member_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.substance_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.part_meronyms())\n",
    "        return meronyms_for_sense\n",
    "    \n",
    "    def get_antonyms_for_sense(self, sense):\n",
    "        antonyms_for_sense = \"\"\n",
    "        for lemma in sense.lemmas():\n",
    "            antonyms_for_sense = antonyms_for_sense + \". \" + self.merge_glosses([ant.synset() for ant in lemma.antonyms()])\n",
    "        return antonyms_for_sense\n",
    "    \n",
    "    def get_similarity_for_sense(self, sense):\n",
    "        similarity_for_sense = self.merge_glosses(sense.similar_tos())\n",
    "        return similarity_for_sense\n",
    "    \n",
    "    def get_entailments_for_sense(self, sense):\n",
    "        entailments_for_sense = self.merge_glosses(sense.entailments())\n",
    "        return entailments_for_sense\n",
    "    \n",
    "    def get_target_sense_score(self, target_sense_tuple):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        for window_word in self.window_words:\n",
    "            window_word_senses = self.get_tuple_senses(window_word)\n",
    "            for window_word_sense in window_word_senses:\n",
    "                relatedness = self.get_relatedness(target_sense_tuple, (window_word_sense, window_word[1]))\n",
    "                target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "            if word_tuple[0] not in stopwords.words('english'): \n",
    "                filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return processed_text[i], i\n",
    "        return None, -1\n",
    "    \n",
    "    # word_tuple = ('mass', 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # tuple_senses = disambiguator.get_tuple_senses(word_tuple)\n",
    "    # print(tuple_senses)\n",
    "    # # [Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n",
    "    def get_tuple_senses(self, word_tuple):\n",
    "        return wn.synsets(word_tuple[0], self.porter_to_wn[word_tuple[1]])\n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    # processed_text = [('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    # stemmed_target = \"mass\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # target_tuple, window_words = disambiguator.get_window_words(processed_text, stemmed_target)\n",
    "    # print(target_tuple)\n",
    "    # print(window_words)\n",
    "    # # ('mass', 'NOUN')\n",
    "    # # [('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    def get_window_words(self, processed_text, stemmed_target):\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_tuple, target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        if target_position == -1:\n",
    "            print(\"Target word is not in the text!\")\n",
    "            return None, -1\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = processed_text[0:target_position] + processed_text[target_position+1:]\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            left_words = processed_text[0:target_position]\n",
    "            right_words = processed_text[target_position+1:2 * self.window_size + 1]\n",
    "            # target_position + self.window_size + 1 + (self.window_size - target_position) =  2 * self.window_size + 1\n",
    "            print(\"Extract everything from the left and (self.window_size - target_position) more from the right\")\n",
    "            window_words=left_words+right_words\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            right_words = processed_text[target_position+1:]\n",
    "            left_words = processed_text[len(processed_text) - 2*self.window_size - 1:target]\n",
    "            # target_position-self.window_size-(self.window_size - (len(processed_text) - target_position - 1)) =\n",
    "            # = -2*self.window_size + target_position - target_position + len(processed_text) - 1 = \n",
    "            # = len(processed_text) - 2*self.window_size - 1\n",
    "            print(\"Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\")\n",
    "            window_words=left_words+right_words\n",
    "        else:\n",
    "            left_words = processed_text[target_position-self.window_size:target_position]\n",
    "            right_words = processed_text[target_position+1:target_position+1+self.window_size]\n",
    "            print(\"Extract self.window_size word tuples from each side\")\n",
    "            window_words=left_words+right_words\n",
    "        return target_tuple, window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.get_processed(\"the mass of the object is ten kilograms\")\n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "        word_pos_pairs = nltk.pos_tag(tokenized_words, tagset='universal')\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    # eg:\n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        target_tuple, self.window_words = self.get_window_words(processed_text, stemmed_target)\n",
    "        if target_tuple == None:\n",
    "            return\n",
    "        target_senses = self.get_tuple_senses(target_tuple)\n",
    "        best_score = (None, 0)\n",
    "        for target_sense in target_senses:\n",
    "            target_sense_score = self.get_target_sense_score((target_sense, target_tuple[1]))\n",
    "            if target_sense_score > best_score[1] :\n",
    "                best_score = (target_sense, target_sense_score)\n",
    "        \n",
    "        disambiguated_sense = self.get_gloss_for_sense(best_score[0])\n",
    "        return disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the property of a body that causes it to have weight in a gravitational field'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disambiguator = Disambiguator()\n",
    "disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "# disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "# disambiguator.disambiguate(\"the angry mass of people went after him\", \"went\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target position is 0\n",
      "('mass', 'NOUN')\n",
      "[('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "processed_text = [('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "stemmed_target = \"mass\"\n",
    "disambiguator = Disambiguator()\n",
    "target_tuple, window_words = disambiguator.get_window_words(processed_text, stemmed_target)\n",
    "print(target_tuple)\n",
    "print(window_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n"
     ]
    }
   ],
   "source": [
    "word_tuple = ('mass', 'NOUN')\n",
    "disambiguator = Disambiguator()\n",
    "tuple_senses = disambiguator.get_tuple_senses(word_tuple)\n",
    "print(tuple_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['a', '*', '*', 'd']\n",
      "['*', '*', 'x', 'c', 'd']\n"
     ]
    }
   ],
   "source": [
    "a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "b = [\"b\", \"c\", \"x\", \"c\", \"d\"]\n",
    "disambiguator = Disambiguator()\n",
    "length, new_a, new_b = disambiguator.get_longest_common_substring(a, b)\n",
    "print(length) # 2\n",
    "print(new_a)  # [\"a\", \"*\", \"*\", \"d\"]\n",
    "print(new_b)  # [\"*\", \"*\", \"x\", \"c\", \"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['a', '*', '*', '*']\n",
      "['*', '*', 'x', 'c', '*']\n"
     ]
    }
   ],
   "source": [
    "length, new_a, new_b = disambiguator.get_longest_common_substring(new_a, new_b)\n",
    "print(length) # 2\n",
    "print(new_a)  # [\"a\", \"*\", \"*\", \"d\"]\n",
    "print(new_b)  # [\"*\", \"*\", \"x\", \"c\", \"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a = \"a b c d\"\n",
    "b = \"b c x c d\"\n",
    "disambiguator = Disambiguator()\n",
    "overlap_score = disambiguator.get_overlap_score(a, b)\n",
    "print(overlap_score) # 5 = 4 + 1 = 2^2 + 1^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('mass.n.01')\n",
      "Synset('kilogram.n.01')\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "mass =  wn.synsets(\"mass\", wn.NOUN)[0]\n",
    "print(mass)\n",
    "kilogram = wn.synsets(\"kilogram\", wn.NOUN)[0]\n",
    "print(kilogram)\n",
    "target_tuple, window_tuple =(mass, 'NOUN'), (kilogram, 'NOUN')\n",
    "disambiguator = Disambiguator()\n",
    "relatedness_score = disambiguator.get_relatedness(target_tuple, window_tuple)\n",
    "print(relatedness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disambiguator = Disambiguator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "Target position is 1\n",
      "[('mass', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "[Synset('object.n.01'), Synset('aim.n.02'), Synset('object.n.03'), Synset('object.n.04'), Synset('object.n.05')]\n",
      "a tangible and visible entity; an entity that can cast a shadow. it was full of rackets, balls and other objects\n",
      "the goal intended to be attained (and which is believed to be attainable). the sole object of her trip was to see her children\n",
      "(grammar) a constituent that is acted upon. the object of the verb\n",
      "the focus of cognitions or feelings. objects of thought. the object of my affection\n",
      "(computing) a discrete item that provides a description of virtually anything known to a computer. in object-oriented programming, objects include data and define its status, its methods of operation and how it interacts with other objects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Implemented Yet'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'Synset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-adad6ecf0a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mass.n.01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'Synset'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
