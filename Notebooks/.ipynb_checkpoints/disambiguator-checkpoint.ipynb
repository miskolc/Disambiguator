{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"the mass of the object is ten kilograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"the mass of the object is ten kilograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(text, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('death.n.04'),\n",
       " Synset('die.v.01'),\n",
       " Synset('die.v.02'),\n",
       " Synset('die.v.03'),\n",
       " Synset('fail.v.04'),\n",
       " Synset('die.v.05'),\n",
       " Synset('die.v.06'),\n",
       " Synset('die.v.07'),\n",
       " Synset('die.v.08'),\n",
       " Synset('die.v.09'),\n",
       " Synset('die.v.10'),\n",
       " Synset('die.v.11'),\n",
       " Synset('dying.a.01'),\n",
       " Synset('anxious.s.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kilograms'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed = (stemmer.stem(pos[7][0]), pos[7][1])\n",
    "stemmed_pairs.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kilogram', 'NOUN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"dying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.porter_to_wn = {\n",
    "            \"NOUN\": wn.NOUN,\n",
    "            \"VERB\": wn.VERB,\n",
    "            \"ADJ\" : wn.ADJ\n",
    "        }\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        \n",
    "    def get_longest_common_subsequence(text_a, text_b):\n",
    "        # !IMPORTANT, after finding the sequence, mark the words from both text_a and text_b\n",
    "        # We do this to avoid using a word in multiple sequences\n",
    "        # eg:\n",
    "        # a b c d\n",
    "        # b c x c d \n",
    "        # the c in the first string will be part of the bc sequence but not of cd\n",
    "        return sequence_length, text_a, text_b\n",
    "        \n",
    "    def get_overlap_score(text_a, text_b):\n",
    "        overlap_score = 0\n",
    "        # while length > 0\n",
    "            # sequence_length, text_a, text_b = get_longest_common_subsequence(text_a, text_b)\n",
    "            # overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "        \n",
    "        return overlap_score\n",
    "    \n",
    "    def get_text_for_sense(self, sense):\n",
    "        text_for_sense = \"\"\n",
    "        text_for_sense = text_for_sense + sense.definition()\n",
    "        for example in sense.examples():\n",
    "            text_for_sense = text_for_sense + \". \" + example\n",
    "        return text_for_sense\n",
    "    \n",
    "    def get_relatedness(sense_a, sense_b):\n",
    "        relatedness = 0\n",
    "        \n",
    "        return relatedness\n",
    "        \n",
    "    def get_target_sense_score(self, target_sense):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        # for each window_word in self.window_words\n",
    "            # get all window_word_senses for the window_word\n",
    "            # for each window_word_sense in window_word_senses\n",
    "                # relatedness = get_relatedness(target_sense, window_word_sense)\n",
    "                # target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "#     def get_window_words(self, text_string):\n",
    "#         print(\"Set self.window_words here\")\n",
    "        # do POS-tagging\n",
    "        # remove words without meaning( the)\n",
    "        # get self.window_size words from the left and from the right\n",
    "            # if not enough words on one side, \n",
    "                # add the rest from the opposite side\n",
    "            # if not enough words on either sides(i.e. len(words(text_string)) < 2 * self.window_size + 1 ) \n",
    "                # just use all words in text_string except the target_word\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "          if word_tuple[0] not in stopwords.words('english'): \n",
    "            filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return processed_text[i], i\n",
    "        return None, -1\n",
    "    \n",
    "    def get_target_senses(self, word_tuple):\n",
    "        return wn.synsets(word_tuple[0], self.porter_to_wn[word_tuple[1]])\n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    \n",
    "    def get_window_words(self, processed_text, stemmed_target):\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_tuple, target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        print(\"Target position is \" + str(target_position))\n",
    "        if target_position == -1:\n",
    "            print(\"Target word is not in the text!\")\n",
    "            return None, -1\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = processed_text[0:target_position] + processed_text[target_position+1:]\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            left_words = processed_text[0:target_position]\n",
    "            right_words = processed_text[target_position+1:2 * self.window_size + 1]\n",
    "            # target_position + self.window_size + 1 + (self.window_size - target_position) =  2 * self.window_size + 1\n",
    "            print(\"Extract everything from the left and (self.window_size - target_position) more from the right\")\n",
    "            window_words=left_words+right_words\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            right_words = processed_text[target_position+1:]\n",
    "            left_words = processed_text[len(processed_text) - 2*self.window_size - 1:target]\n",
    "            # target_position-self.window_size-(self.window_size - (len(processed_text) - target_position - 1)) =\n",
    "            # = -2*self.window_size + target_position - target_position + len(processed_text) - 1 = \n",
    "            # = len(processed_text) - 2*self.window_size - 1\n",
    "            print(\"Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\")\n",
    "            window_words=left_words+right_words\n",
    "        else:\n",
    "            left_words = processed_text[target_position-self.window_size:target_position]\n",
    "            right_words = processed_text[target_position+1:target_position+1+self.window_size]\n",
    "            print(\"Extract self.window_size word tuples from each side\")\n",
    "            window_words=left_words+right_words\n",
    "        return target_tuple, window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "        word_pos_pairs = nltk.pos_tag(text, tagset='universal')\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    \n",
    "    # eg:\n",
    "    # > disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # > disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "        print(processed_text)\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        target_tuple, window_words = self.get_window_words(processed_text, stemmed_target)\n",
    "        if target_tuple == None:\n",
    "            return\n",
    "        print(window_words)\n",
    "        target_senses = self.get_target_senses(target_tuple)\n",
    "        print(target_senses)\n",
    "        for target_sense in target_senses:\n",
    "            print(self.get_text_for_sense(target_sense))\n",
    "        # for each target_sense in target_senses\n",
    "            # calculate the target_sense_score\n",
    "        # take the target_sense with the \n",
    "        return disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disambiguator = Disambiguator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "Target position is 0\n",
      "[('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "[Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n",
      "the property of a body that causes it to have weight in a gravitational field\n",
      "(often followed by `of') a large number or amount or extent. a batch of letters. a deal of trouble. a lot of money. he made a mint on the stock market. see the rest of the winners in our huge passel of photos. it must have cost plenty. a slew of journalists. a wad of money\n",
      "an ill-structured collection of similar things (objects or people)\n",
      "(Roman Catholic Church and Protestant Churches) the celebration of the Eucharist\n",
      "a body of matter without definite shape. a huge ice mass\n",
      "the common people generally. separate the warriors from the mass. power to the people\n",
      "the property of something that is great in magnitude. it is cheaper to buy it in bulk. he received a mass of correspondence. the volume of exports\n",
      "a musical setting for a Mass. they played a Mass composed by Beethoven\n",
      "a sequence of prayers constituting the Christian Eucharistic rite. the priest said Mass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Implemented Yet'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'Synset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-adad6ecf0a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mass.n.01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'Synset'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
