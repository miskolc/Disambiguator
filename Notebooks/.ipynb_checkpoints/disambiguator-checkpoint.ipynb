{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"the mass of the object is ten kilograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"the mass of the object is ten kilograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(text, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('death.n.04'),\n",
       " Synset('die.v.01'),\n",
       " Synset('die.v.02'),\n",
       " Synset('die.v.03'),\n",
       " Synset('fail.v.04'),\n",
       " Synset('die.v.05'),\n",
       " Synset('die.v.06'),\n",
       " Synset('die.v.07'),\n",
       " Synset('die.v.08'),\n",
       " Synset('die.v.09'),\n",
       " Synset('die.v.10'),\n",
       " Synset('die.v.11'),\n",
       " Synset('dying.a.01'),\n",
       " Synset('anxious.s.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kilograms'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed = (stemmer.stem(pos[7][0]), pos[7][1])\n",
    "stemmed_pairs.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kilogram', 'NOUN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"dying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        \n",
    "    def get_longest_common_subsequence(text_a, text_b):\n",
    "        # !IMPORTANT, after finding the sequence, mark the words from both text_a and text_b\n",
    "        # We do this to avoid using a word in multiple sequences\n",
    "        # eg:\n",
    "        # a b c d\n",
    "        # b c x c d \n",
    "        # the c in the first string will be part of the bc sequence but not of cd\n",
    "        return sequence_length, text_a, text_b\n",
    "        \n",
    "    def get_overlap_score(text_a, text_b):\n",
    "        overlapt_score = 0\n",
    "        # while length > 0\n",
    "            # sequence_length, text_a, text_b = get_longest_common_subsequence(text_a, text_b)\n",
    "            # overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "        \n",
    "        return overlap_score\n",
    "        \n",
    "    def get_relatedness(sense_a, sense_b):\n",
    "        relatedness = 0\n",
    "        \n",
    "        return relatedness\n",
    "        \n",
    "    def get_target_sense_score(self, target_sense):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        # for each window_word in self.window_words\n",
    "            # get all window_word_senses for the window_word\n",
    "            # for each window_word_sense in window_word_senses\n",
    "                # relatedness = get_relatedness(target_sense, window_word_sense)\n",
    "                # target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "#     def get_window_words(self, text_string):\n",
    "#         print(\"Set self.window_words here\")\n",
    "        # do POS-tagging\n",
    "        # remove words without meaning( the)\n",
    "        # get self.window_size words from the left and from the right\n",
    "            # if not enough words on one side, \n",
    "                # add the rest from the opposite side\n",
    "            # if not enough words on either sides(i.e. len(words(text_string)) < 2 * self.window_size + 1 ) \n",
    "                # just use all words in text_string except the target_word\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "          if word_tuple[0] not in stopwords.words('english'): \n",
    "            filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    def get_window_words(self, processed_text, target_word):\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        print(\"Target position is \" + str(target_position))\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = self.remove_target(processed_text, target_position)\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            print(\"Extract everything from the left and (target_position - self.window_size) more from the right\")\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            print(\"Extract everything from the right and (self.num_words_on_the_right - self.window_size) more from the left\")\n",
    "        else:\n",
    "            print(\"Extract self.window_size word tuples from each side\")\n",
    "        return window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "        word_pos_pairs = nltk.pos_tag(text, tagset='universal')\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    \n",
    "    # eg:\n",
    "    # > disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # > disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "        print(processed_text)\n",
    "        window_words = self.get_window_words(processed_text, target_word)\n",
    "        print(window_words)\n",
    "        # get window_words for self.window_size\n",
    "        # get all senses for the target_word\n",
    "        # for each target_sense in target_senses\n",
    "            # calculate the target_sense_score\n",
    "        # take the target_sense with the \n",
    "        return disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disambiguator = Disambiguator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
      "Target position is 0\n",
      "[('mass', 'NOUN'), ('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not Implemented Yet'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
