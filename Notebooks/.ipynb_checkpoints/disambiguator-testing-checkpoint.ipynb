{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import senseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size=3):\n",
    "        self.window_size = window_size\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.porter_to_wn = {\n",
    "            \"NOUN\": wn.NOUN,\n",
    "            \"VERB\": wn.VERB,\n",
    "            \"ADJ\" : wn.ADJ\n",
    "        }\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        self.rel_pairs = {\n",
    "            \"NOUN\": [\n",
    "                (\"gloss\", \"gloss\"), (\"hypo\", \"hypo\"), (\"mero\", \"mero\"),\n",
    "                (\"gloss\", \"hypo\"), (\"gloss\", \"mero\"), (\"hypo\", \"mero\"),\n",
    "                (\"hypo\", \"gloss\"), (\"mero\", \"gloss\"), (\"mero\", \"hypo\")\n",
    "            ],\n",
    "            \"ADJ\" : [\n",
    "                (\"gloss\", \"gloss\"), (\"anto\", \"anto\"), (\"sim\", \"sim\"),\n",
    "                (\"gloss\", \"anto\"), (\"gloss\", \"sim\"), (\"anto\", \"sim\"),\n",
    "                (\"anto\", \"gloss\"), (\"sim\", \"gloss\"), (\"sim\", \"anto\")\n",
    "            ], \n",
    "            \"VERB\": [\n",
    "                (\"gloss\", \"gloss\"), (\"entl\", \"entl\"),\n",
    "                (\"gloss\", \"entl\"), (\"entl\", \"gloss\")\n",
    "            ],\n",
    "            \"default\": [(\"gloss\", \"gloss\")]\n",
    "        }\n",
    "        \n",
    "        \n",
    "    # a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "    # b = [\"b\", \"c\", \"x\", \"c\", \"d\"]\n",
    "    # disambiguator = Disambiguator()\n",
    "    # length, new_a, new_b = disambiguator.get_longest_common_substring(a, b)\n",
    "    # print(length) # 2\n",
    "    # print(new_a)  # [\"a\", \"*\", \"*\", \"d\"]\n",
    "    # print(new_b)  # [\"*\", \"*\", \"x\", \"c\", \"d\"]\n",
    "    def get_longest_common_substring(self, tokens_a, tokens_b):\n",
    "        lcs = [[0] * (1 + len(tokens_b)) for i in range(1 + len(tokens_a))]\n",
    "        best, best_position_a, best_position_b = 0, 0, 0\n",
    "        for i in range(1, 1 + len(tokens_a)):\n",
    "            for j in range(1, 1 + len(tokens_b)):\n",
    "                if (tokens_a[i - 1] == tokens_b[j - 1]) & (tokens_a[i-1] != \"*\"):\n",
    "                    lcs[i][j] = lcs[i - 1][j - 1] + 1\n",
    "                    if lcs[i][j] > best:\n",
    "                        best = lcs[i][j]\n",
    "                        best_position_a = i\n",
    "                        best_position_b = j\n",
    "                else:\n",
    "                    lcs[i][j] = 0\n",
    "                    \n",
    "        tokens_a[best_position_a - best:best_position_a] = \"*\" * best\n",
    "        tokens_b[best_position_b - best:best_position_b] = \"*\" * best\n",
    "        \n",
    "        return best, tokens_a, tokens_b\n",
    "    \n",
    "    # a = \"a b c d\"\n",
    "    # b = \"b c x c d\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # overlap_score = disambiguator.get_overlap_score(a, b)\n",
    "    # print(overlap_score) # 5 = 4 + 1 = 2^2 + 1^2\n",
    "    def get_overlap_score(self, text_a, text_b):\n",
    "        overlap_score = 0\n",
    "        tokens_a = nltk.word_tokenize(text_a)\n",
    "        tokens_b = nltk.word_tokenize(text_b)\n",
    "        sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        while sequence_length > 0:\n",
    "            overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "            sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        \n",
    "        return overlap_score\n",
    "    \n",
    "    def get_texts(self, target_tuple, window_tuple):\n",
    "        target = {}\n",
    "        window = {}\n",
    "        \n",
    "        target[\"gloss\"] = self.get_gloss_for_sense(target_tuple[0])\n",
    "        window[\"gloss\"] = self.get_gloss_for_sense(window_tuple[0])\n",
    "        target[\"hypo\"] = self.get_hyponyms_for_sense(target_tuple[0])\n",
    "        window[\"hypo\"] = self.get_hyponyms_for_sense(window_tuple[0])\n",
    "        target[\"mero\"] = self.get_meronyms_for_sense(target_tuple[0])\n",
    "        window[\"mero\"] = self.get_meronyms_for_sense(window_tuple[0])\n",
    "        target[\"anto\"] = self.get_antonyms_for_sense(target_tuple[0])\n",
    "        window[\"anto\"] = self.get_antonyms_for_sense(window_tuple[0])\n",
    "        target[\"sim\"] = self.get_similarity_for_sense(target_tuple[0])\n",
    "        window[\"sim\"] = self.get_similarity_for_sense(window_tuple[0])\n",
    "        target[\"entl\"] = self.get_entailments_for_sense(target_tuple[0])\n",
    "        window[\"entl\"] = self.get_entailments_for_sense(window_tuple[0])\n",
    "        \n",
    "        return target, window\n",
    "    \n",
    "    def get_enhanced_relatedness(self, target_pos, target_texts, window_texts):\n",
    "        relatedness = 0\n",
    "        for rel_pair in self.rel_pairs[target_pos]:\n",
    "            relatedness = relatedness + self.get_overlap_score(target_texts[rel_pair[0]], window_texts[rel_pair[1]])\n",
    "        return relatedness\n",
    "    \n",
    "    \n",
    "    # mass =  wn.synsets(\"mass\", wn.NOUN)[0]\n",
    "    # print(mass)\n",
    "    # kilogram = wn.synsets(\"kilogram\", wn.NOUN)[0]\n",
    "    # print(kilogram)\n",
    "    # target_tuple, window_tuple =(mass, 'NOUN'), (kilogram, 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # relatedness_score = disambiguator.get_relatedness(target_tuple, window_tuple)\n",
    "    # print(relatedness_score)\n",
    "    def get_relatedness(self, target_tuple, window_tuple):\n",
    "        target_texts, window_texts = self.get_texts(target_tuple, window_tuple)\n",
    "        if target_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "            relatedness = self.get_enhanced_relatedness(target_tuple[1], target_texts, window_texts)\n",
    "        else:\n",
    "            relatedness = self.get_overlap_score(\"default\", target_texts, window_texts)\n",
    "            \n",
    "        return relatedness\n",
    "    \n",
    "    def get_gloss_for_sense(self, sense):\n",
    "        gloss_for_sense = \"\"\n",
    "        gloss_for_sense = gloss_for_sense + sense.definition()\n",
    "        for example in sense.examples():\n",
    "            gloss_for_sense =gloss_for_sense + \". \" + example\n",
    "        return gloss_for_sense\n",
    "    \n",
    "    def merge_glosses(self, synsets):\n",
    "        aggregator = \"\"\n",
    "        for synset in synsets:\n",
    "            aggregator = aggregator + \". \" + self.get_gloss_for_sense(synset)\n",
    "        return aggregator\n",
    "    \n",
    "    def get_hyponyms_for_sense(self, sense):\n",
    "        hyponyms_for_sense = self.merge_glosses(sense.hyponyms())\n",
    "        return hyponyms_for_sense\n",
    "    \n",
    "    def get_meronyms_for_sense(self, sense):\n",
    "        meronyms_for_sense = self.merge_glosses(sense.member_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.substance_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.part_meronyms())\n",
    "        return meronyms_for_sense\n",
    "    \n",
    "    def get_antonyms_for_sense(self, sense):\n",
    "        antonyms_for_sense = \"\"\n",
    "        for lemma in sense.lemmas():\n",
    "            antonyms_for_sense = antonyms_for_sense + \". \" + self.merge_glosses([ant.synset() for ant in lemma.antonyms()])\n",
    "        return antonyms_for_sense\n",
    "    \n",
    "    def get_similarity_for_sense(self, sense):\n",
    "        similarity_for_sense = self.merge_glosses(sense.similar_tos())\n",
    "        return similarity_for_sense\n",
    "    \n",
    "    def get_entailments_for_sense(self, sense):\n",
    "        entailments_for_sense = self.merge_glosses(sense.entailments())\n",
    "        return entailments_for_sense\n",
    "    \n",
    "    def get_target_sense_score(self, target_sense_tuple):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        for window_word in self.window_words:\n",
    "            #print(window_word)\n",
    "            window_word_senses = self.get_tuple_senses(window_word)\n",
    "            for window_word_sense in window_word_senses:\n",
    "                relatedness = self.get_relatedness(target_sense_tuple, (window_word_sense, window_word[1]))\n",
    "                target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "            if word_tuple[0] not in stopwords.words('english'): \n",
    "                if word_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "                    filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def remove_punctuation(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text:  \n",
    "            if (len(word_tuple) == 2):\n",
    "                if (word_tuple[1] != \".\") : # word_tuple[0] not in [\".\", \"`\"]: \n",
    "                    filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return processed_text[i], i\n",
    "        return None, -1\n",
    "    \n",
    "    # word_tuple = ('mass', 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # tuple_senses = disambiguator.get_tuple_senses(word_tuple)\n",
    "    # print(tuple_senses)\n",
    "    # # [Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n",
    "    def get_tuple_senses(self, word_tuple):\n",
    "        return wn.synsets(word_tuple[0], self.porter_to_wn[word_tuple[1]])\n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    # processed_text = [('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    # stemmed_target = \"mass\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # target_tuple, window_words = disambiguator.get_window_words(processed_text, stemmed_target)\n",
    "    # print(target_tuple)\n",
    "    # print(window_words)\n",
    "    # # ('mass', 'NOUN')\n",
    "    # # [('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    def get_window_words(self, processed_text, stemmed_target):\n",
    "        processed_text = self.remove_punctuation(processed_text)\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_tuple, target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        if target_position == -1:\n",
    "            print(\"Target word is not in the text!\")\n",
    "            return None, -1\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = processed_text[0:target_position] + processed_text[target_position+1:]\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            left_words = processed_text[0:target_position]\n",
    "            right_words = processed_text[target_position+1:2 * self.window_size + 1]\n",
    "            # target_position + self.window_size + 1 + (self.window_size - target_position) =  2 * self.window_size + 1\n",
    "            # print(\"Extract everything from the left and (self.window_size - target_position) more from the right\")\n",
    "            window_words=left_words+right_words\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            right_words = processed_text[target_position+1:]\n",
    "            left_words = processed_text[len(processed_text) - 2*self.window_size - 1:target_position]\n",
    "            # target_position-self.window_size-(self.window_size - (len(processed_text) - target_position - 1)) =\n",
    "            # = -2*self.window_size + target_position - target_position + len(processed_text) - 1 = \n",
    "            # = len(processed_text) - 2*self.window_size - 1\n",
    "            # print(\"Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\")\n",
    "            window_words=left_words+right_words\n",
    "        else:\n",
    "            left_words = processed_text[target_position-self.window_size:target_position]\n",
    "            right_words = processed_text[target_position+1:target_position+1+self.window_size]\n",
    "            # print(\"Extract self.window_size word tuples from each side\")\n",
    "            window_words=left_words+right_words\n",
    "        return target_tuple, window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.get_processed(\"the mass of the object is ten kilograms\")\n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "#         print(\"Tokens with punctuation\")\n",
    "#         print(tokenized_words_with_punctuation)\n",
    "#         tokenized_words = self.remove_punctuation(tokenized_words_with_punctuation)\n",
    "#         print(\"Tokens\")\n",
    "#         print(tokenized_words)\n",
    "        word_pos_pairs = nltk.pos_tag(tokenized_words, tagset='universal')\n",
    "#         print(\"POS\")\n",
    "#         print(word_pos_pairs)\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "#         print(\"Stemmed\")\n",
    "#         print(stemmed_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    # eg:\n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "#         print(processed_text)\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        \n",
    "#         print(target_word)\n",
    "#         print(stemmed_target)\n",
    "        target_tuple, self.window_words = self.get_window_words(processed_text, stemmed_target)\n",
    "        if target_tuple == None:\n",
    "            return\n",
    "#         print(target_tuple)\n",
    "        target_senses = self.get_tuple_senses((target_word, target_tuple[1]))\n",
    "        best_score = (None, -1)\n",
    "#         print(target_senses)\n",
    "        for target_sense in target_senses:\n",
    "            target_sense_score = self.get_target_sense_score((target_sense, target_tuple[1]))\n",
    "            if target_sense_score > best_score[1] :\n",
    "                best_score = (target_sense, target_sense_score)\n",
    "        \n",
    "        predicted_synset = best_score[0]\n",
    "        disambiguated_sense = self.get_gloss_for_sense(best_score[0])\n",
    "        return predicted_synset, disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DisambiguatorTester:\n",
    "    def __init__(self, window_size=3):\n",
    "        self.disambiguator = Disambiguator(window_size)\n",
    "        self.sense_map = {\n",
    "            \"HARD1\": [\"difficult.a.01\"],    # not easy, requiring great physical or mental\n",
    "            \"HARD2\": [\"hard.a.02\",          # dispassionate\n",
    "                      \"difficult.a.01\"],\n",
    "            \"HARD3\": [\"hard.a.03\"],         # resisting weight or pressure\n",
    "            \"interest_1\": [\"interest.n.01\"], # readiness to give attention\n",
    "            \"interest_2\": [\"interest.n.03\"], # quality of causing attention to be given to\n",
    "            \"interest_3\": [\"pastime.n.01\"],  # activity, etc. that one gives attention to\n",
    "            \"interest_4\": [\"sake.n.01\"],     # advantage, advancement or favor\n",
    "            \"interest_5\": [\"interest.n.05\"], # a share in a company or business\n",
    "            \"interest_6\": [\"interest.n.04\"], # money paid for the use of money\n",
    "            \"cord\": [\"line.n.18\"],          # something (as a cord or rope) that is long and thin and flexible\n",
    "            \"formation\": [\"line.n.01\",\"line.n.03\"], # a formation of people or things one beside another\n",
    "            \"text\": [\"line.n.05\"],                 # text consisting of a row of words written across a page or computer screen\n",
    "            \"phone\": [\"telephone_line.n.02\"],   # a telephone connection\n",
    "            \"product\": [\"line.n.22\"],       # a particular kind of product or merchandise\n",
    "            \"division\": [\"line.n.29\"],      # a conceptual separation or distinction\n",
    "            \"SERVE12\": [\"serve.v.02\"],       # do duty or hold offices; serve in a specific function\n",
    "            \"SERVE10\": [\"serve.v.06\"], # provide (usually but not necessarily food)\n",
    "            \"SERVE2\": [\"serve.v.01\"],       # serve a purpose, role, or function\n",
    "            \"SERVE6\": [\"service.v.01\"]      # be used by; as of a utility\n",
    "        }\n",
    "        \n",
    "    def get_test_case(self, instance):\n",
    "        pos = instance.position\n",
    "        target_word = ' '.join(w for (w,t) in instance.context[pos:pos+1])\n",
    "        \n",
    "#         print(instance.context[0:pos])\n",
    "        left = ' '.join( \"\" if word_tuple=='FRASL' else word_tuple[0] for word_tuple in instance.context[0:pos]) # .items()\n",
    "        right = ' '.join(w for (w,t) in instance.context[pos+1:])\n",
    "        phrase = left + \" \" + target_word + \" \" + right\n",
    "        \n",
    "        target_synsets_names = self.sense_map[instance.senses[0]]\n",
    "        target_synsets = [wn.synset(name) for name in target_synsets_names]\n",
    "        \n",
    "        return phrase, target_word, target_synsets\n",
    "        \n",
    "    def test(self):\n",
    "        correct_predictions, total_tests = 0, 0\n",
    "        corpuses = senseval.fileids()\n",
    "        for corpus in corpuses[2:]:\n",
    "            print(\"=\" * 100)\n",
    "#             no_instances = len(senseval.instances(corpus))\n",
    "#             print(\"Testing \" + corpus + \" with \" + str(no_instances) + \" instances.\" )\n",
    "            print(\"Testing \" + corpus)\n",
    "            print(\"=\" * 100)\n",
    "            for instance in senseval.instances(corpus)[0:2]:\n",
    "                phrase, target_word, target_synsets = self.get_test_case(instance)\n",
    "                print(\"-\" * 100)\n",
    "                print(\"Phrase: \" + phrase)\n",
    "                print(\"_\" * 5)\n",
    "                print(\"Target Word: \" + target_word)\n",
    "                print(\"_\" * 5)\n",
    "                print(\"Target Synsets: \" + str(target_synsets) + \" meaning \" + target_synsets[0].definition())\n",
    "                print(\"_\" * 5)\n",
    "                predicted_synset, description = self.disambiguator.disambiguate(phrase, target_word)\n",
    "                print(\"Predicted Synset: \" + str(predicted_synset) + \" meaning: \")\n",
    "                print(description)\n",
    "                if predicted_synset in target_synsets:\n",
    "                    correct_predictions += 1\n",
    "                total_tests +=1\n",
    "        print(\"=\" * 100)\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Run \" + str(total_tests) + \" tests\")\n",
    "        print(\"Predicted \" + str(correct_predictions) + \" predictions\")\n",
    "        print(\"Percentage \" + str(correct_predictions / total_tests * 100) + \"%\")\n",
    "        print(\"=\" * 100)\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tester = DisambiguatorTester(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Testing line.pos\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: the company argued that its foreman needn 't have told the worker not to move the plank to which his lifeline was tied because \" that comes with common sense . \" the commission noted , however , that dellovade hadn 't instructed its employees on how to secure their lifelines and didn 't heed a federal inspector 's earlier suggestion that the company install special safety lines inside the a-frame structure it was building .\n",
      "_____\n",
      "Target Word: lines\n",
      "_____\n",
      "Target Synsets: [Synset('line.n.18')] meaning something (as a cord or rope) that is long and thin and flexible\n",
      "_____\n",
      "Predicted Synset: Synset('line.n.18') meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible. a washing line\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: the set , designed by mr . hall 's longtime associate eugene lee , has the audience divided in half , facing a central playing area . off to one side -- representing the \" have-nots \" of louisiana -- is a broken-down shack with a woodpile and a wash line .\n",
      "_____\n",
      "Target Word: line\n",
      "_____\n",
      "Target Synsets: [Synset('line.n.18')] meaning something (as a cord or rope) that is long and thin and flexible\n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x +=1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrase = \"`` he may lose all popular support , but someone has to kill him to defeat him and that 's hard to do . ''\"\n",
    "target_word = \"hard\"\n",
    "diz = Disambiguator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard\n",
      "hard\n",
      "Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-4f41652388df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisambiguate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-146-f71c43a1037c>\u001b[0m in \u001b[0;36mdisambiguate\u001b[0;34m(self, text_string, target_word)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget_sense\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_senses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mtarget_sense_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_sense_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarget_sense_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_sense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sense_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-f71c43a1037c>\u001b[0m in \u001b[0;36mget_target_sense_score\u001b[0;34m(self, target_sense_tuple)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwindow_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mwindow_word_senses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tuple_senses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwindow_word_sense\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindow_word_senses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mrelatedness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relatedness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sense_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindow_word_sense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-f71c43a1037c>\u001b[0m in \u001b[0;36mget_tuple_senses\u001b[0;34m(self, word_tuple)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# # [Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_tuple_senses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter_to_wn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnum_words_on_the_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "diz.disambiguate(phrase, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Synset('object.n.01'),\n",
       " 'a tangible and visible entity; an entity that can cast a shadow. it was full of rackets, balls and other objects')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diz.disambiguate(\"the mass of the object is ten kilograms\", \"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n"
     ]
    }
   ],
   "source": [
    "word_tuple = ('mass', 'NOUN')\n",
    "disambiguator = Disambiguator()\n",
    "tuple_senses = disambiguator.get_tuple_senses(word_tuple)\n",
    "print(tuple_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass\n",
      "mass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Synset('mass.n.01'),\n",
       " 'the property of a body that causes it to have weight in a gravitational field')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disambiguator = Disambiguator()\n",
    "disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard\n",
      "hard\n",
      "Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-3a6a1afb5c8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisambiguator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisambiguate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"he may lose all popular support , but someone has to kill him to defeat him and that 's hard to do\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-146-f71c43a1037c>\u001b[0m in \u001b[0;36mdisambiguate\u001b[0;34m(self, text_string, target_word)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget_sense\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_senses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mtarget_sense_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_sense_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarget_sense_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_sense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sense_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-f71c43a1037c>\u001b[0m in \u001b[0;36mget_target_sense_score\u001b[0;34m(self, target_sense_tuple)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwindow_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mwindow_word_senses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tuple_senses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwindow_word_sense\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindow_word_senses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mrelatedness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relatedness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sense_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindow_word_sense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-f71c43a1037c>\u001b[0m in \u001b[0;36mget_tuple_senses\u001b[0;34m(self, word_tuple)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# # [Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_tuple_senses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter_to_wn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnum_words_on_the_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "disambiguator.disambiguate(\"he may lose all popular support , but someone has to kill him to defeat him and that 's hard to do\", \"hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tup = (\"x\",\"y\",\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\".\" != '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'he',\n",
       " 'may',\n",
       " 'lose',\n",
       " 'all',\n",
       " 'popular',\n",
       " 'support',\n",
       " ',',\n",
       " 'but',\n",
       " 'someone',\n",
       " 'has',\n",
       " 'to',\n",
       " 'kill',\n",
       " 'him',\n",
       " 'to',\n",
       " 'defeat',\n",
       " 'him',\n",
       " 'and',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'hard',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"`` he may lose all popular support , but someone has to kill him to defeat him and that 's hard to do . ''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'ADP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-8d177ef3f61c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inside\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'ADP'"
     ]
    }
   ],
   "source": [
    "wn.synsets(\"inside\", wn.ADP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'serve.pos'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval.fileids()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('serve.n.01'),\n",
       " Synset('serve.v.01'),\n",
       " Synset('serve.v.02'),\n",
       " Synset('serve.v.03'),\n",
       " Synset('service.v.01'),\n",
       " Synset('serve.v.05'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('serve.v.07'),\n",
       " Synset('serve.v.08'),\n",
       " Synset('serve.v.09'),\n",
       " Synset('serve.v.10'),\n",
       " Synset('serve.v.11'),\n",
       " Synset('suffice.v.01'),\n",
       " Synset('serve.v.13'),\n",
       " Synset('serve.v.14'),\n",
       " Synset('serve.v.15')]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
