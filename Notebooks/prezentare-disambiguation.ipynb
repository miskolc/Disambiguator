{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import senseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Disambiguator:\n",
    "    \n",
    "    def __init__(self, window_size=3, rel_pairs_set=\"old\"):\n",
    "        self.window_size = window_size\n",
    "        self.rel_pairs_set = rel_pairs_set\n",
    "        self.window_words = []\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.porter_to_wn = {\n",
    "            \"NOUN\": wn.NOUN,\n",
    "            \"VERB\": wn.VERB,\n",
    "            \"ADJ\" : wn.ADJ\n",
    "        }\n",
    "        # daca au parte de vorbire diferita folosim doar glossesle\n",
    "        # RELPAIRS\n",
    "            # subst: gloss, hiponim, meronim\n",
    "            # adj: gloss, antonim, similarity\n",
    "            # verb: gloss, entailment\n",
    "        self.rel_pairs = {\n",
    "            \"older\" : {\n",
    "                \"NOUN\": [\n",
    "                    (\"gloss\", \"gloss\"), (\"hypo\", \"hypo\"), (\"mero\", \"mero\"),\n",
    "                    (\"gloss\", \"hypo\"), (\"gloss\", \"mero\"), (\"hypo\", \"mero\"),\n",
    "                    (\"hypo\", \"gloss\"), (\"mero\", \"gloss\") , (\"mero\", \"hypo\")\n",
    "                    # fara (\"hypo\", \"mero\"), (\"mero\", \"hypo\")\n",
    "                ],\n",
    "                \"ADJ\" : [\n",
    "                    (\"gloss\", \"gloss\"), (\"anto\", \"anto\"), (\"sim\", \"sim\"),\n",
    "                    (\"gloss\", \"anto\"), (\"gloss\", \"sim\"), (\"anto\", \"sim\"),\n",
    "                    (\"anto\", \"gloss\"), (\"sim\", \"gloss\"), (\"sim\", \"anto\")\n",
    "                ], # attribute, also-see\n",
    "                \"VERB\": [\n",
    "                    (\"gloss\", \"gloss\"), (\"entl\", \"entl\"),\n",
    "                    (\"gloss\", \"entl\"), (\"entl\", \"gloss\")\n",
    "                ], # cauzalitate  # caussal relation, dar fara cauzal cu cauzal\n",
    "                \"default\": [(\"gloss\", \"gloss\")]\n",
    "            },\n",
    "            \"old\" : {\n",
    "                \"NOUN\": [\n",
    "                    (\"gloss\", \"gloss\"), (\"hypo\", \"hypo\"), (\"mero\", \"mero\"),\n",
    "                    (\"gloss\", \"hypo\"), (\"gloss\", \"mero\"),\n",
    "                    (\"hypo\", \"gloss\"), (\"mero\", \"gloss\"),\n",
    "                    # fara (\"hypo\", \"mero\"), (\"mero\", \"hypo\")\n",
    "                ],\n",
    "                \"ADJ\" : [\n",
    "                    (\"gloss\", \"gloss\"), (\"anto\", \"anto\"), (\"sim\", \"sim\"),\n",
    "                    (\"gloss\", \"anto\"), (\"gloss\", \"sim\"), (\"anto\", \"sim\"),\n",
    "                    (\"anto\", \"gloss\"), (\"sim\", \"gloss\"), (\"sim\", \"anto\")\n",
    "                ], # attribute, also-see\n",
    "                \"VERB\": [\n",
    "                    (\"gloss\", \"gloss\"), (\"entl\", \"entl\"),\n",
    "                    (\"gloss\", \"entl\"), (\"entl\", \"gloss\")\n",
    "                ], # cauzalitate  # caussal relation, dar fara cauzal cu cauzal\n",
    "                \"default\": [(\"gloss\", \"gloss\")]\n",
    "            },\n",
    "            \"new\" : {\n",
    "                \"NOUN\": [\n",
    "                    (\"gloss\", \"gloss\"), (\"hypo\", \"hypo\"), (\"mero\", \"mero\"),\n",
    "                    (\"gloss\", \"hypo\"), (\"gloss\", \"mero\"),\n",
    "                    (\"hypo\", \"gloss\"), (\"mero\", \"gloss\"),\n",
    "                ],\n",
    "                \"ADJ\" : [\n",
    "                    (\"gloss\", \"gloss\"), (\"anto\", \"anto\"), (\"sim\", \"sim\"), (\"also_sees\", \"also_sees\"),\n",
    "                    (\"gloss\", \"anto\"), (\"gloss\", \"sim\"), (\"anto\", \"sim\"),\n",
    "                    (\"anto\", \"gloss\"), (\"sim\", \"gloss\"), (\"sim\", \"anto\")\n",
    "                ],\n",
    "                \"VERB\": [\n",
    "                    (\"gloss\", \"gloss\"), (\"entl\", \"entl\"),\n",
    "                    (\"gloss\", \"entl\"), (\"entl\", \"gloss\"),\n",
    "                    (\"gloss\", \"causes\"), (\"causes\", \"gloss\")\n",
    "                ], # cauzalitate  # causal relation, dar fara cauzal cu cauzal\n",
    "                \"default\": [(\"gloss\", \"gloss\")]\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        \n",
    "    # a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "    # b = [\"b\", \"c\", \"x\", \"c\", \"d\"]\n",
    "    # disambiguator = Disambiguator()\n",
    "    # length, new_a, new_b = disambiguator.get_longest_common_substring(a, b)\n",
    "    # print(length) # 2\n",
    "    # print(new_a)  # [\"a\", \"*\", \"*\", \"d\"]\n",
    "    # print(new_b)  # [\"*\", \"*\", \"x\", \"c\", \"d\"]\n",
    "    def get_longest_common_substring(self, tokens_a, tokens_b):\n",
    "        lcs = [[0] * (1 + len(tokens_b)) for i in range(1 + len(tokens_a))]\n",
    "        best, best_position_a, best_position_b = 0, 0, 0\n",
    "        for i in range(1, 1 + len(tokens_a)):\n",
    "            for j in range(1, 1 + len(tokens_b)):\n",
    "                if (tokens_a[i - 1] == tokens_b[j - 1]) & (tokens_a[i-1] != \"*\"):\n",
    "                    lcs[i][j] = lcs[i - 1][j - 1] + 1\n",
    "                    if lcs[i][j] > best:\n",
    "                        best = lcs[i][j]\n",
    "                        best_position_a = i\n",
    "                        best_position_b = j\n",
    "                else:\n",
    "                    lcs[i][j] = 0\n",
    "                    \n",
    "        tokens_a[best_position_a - best:best_position_a] = \"*\" * best\n",
    "        tokens_b[best_position_b - best:best_position_b] = \"*\" * best\n",
    "        \n",
    "        return best, tokens_a, tokens_b\n",
    "    \n",
    "    # a = \"a b c d\"\n",
    "    # b = \"b c x c d\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # overlap_score = disambiguator.get_overlap_score(a, b)\n",
    "    # print(overlap_score) # 5 = 4 + 1 = 2^2 + 1^2\n",
    "    def get_overlap_score(self, text_a, text_b):\n",
    "        overlap_score = 0\n",
    "        tokens_a = nltk.word_tokenize(text_a)\n",
    "        tokens_b = nltk.word_tokenize(text_b)\n",
    "        sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        while sequence_length > 0:\n",
    "            overlap_score = overlap_score + sequence_length * sequence_length # we square the length \n",
    "            sequence_length, tokens_a, tokens_b = self.get_longest_common_substring(tokens_a, tokens_b)\n",
    "        \n",
    "        return overlap_score\n",
    "    \n",
    "    def get_texts(self, target_tuple, window_tuple):\n",
    "        target = {}\n",
    "        window = {}\n",
    "        \n",
    "        target[\"gloss\"] = self.get_gloss_for_sense(target_tuple[0])\n",
    "        window[\"gloss\"] = self.get_gloss_for_sense(window_tuple[0])\n",
    "        target[\"hypo\"] = self.get_hyponyms_for_sense(target_tuple[0])\n",
    "        window[\"hypo\"] = self.get_hyponyms_for_sense(window_tuple[0])\n",
    "        target[\"mero\"] = self.get_meronyms_for_sense(target_tuple[0])\n",
    "        window[\"mero\"] = self.get_meronyms_for_sense(window_tuple[0])\n",
    "        target[\"anto\"] = self.get_antonyms_for_sense(target_tuple[0])\n",
    "        window[\"anto\"] = self.get_antonyms_for_sense(window_tuple[0])\n",
    "        target[\"sim\"] = self.get_similarity_for_sense(target_tuple[0])\n",
    "        window[\"sim\"] = self.get_similarity_for_sense(window_tuple[0])\n",
    "        target[\"entl\"] = self.get_entailments_for_sense(target_tuple[0])\n",
    "        window[\"entl\"] = self.get_entailments_for_sense(window_tuple[0])\n",
    "        target[\"also_sees\"] = self.get_also_sees_for_sense(target_tuple[0])\n",
    "        window[\"also_sees\"] = self.get_also_sees_for_sense(window_tuple[0])\n",
    "        target[\"causes\"] = self.get_causes_for_sense(target_tuple[0])\n",
    "        window[\"causes\"] = self.get_causes_for_sense(window_tuple[0])\n",
    "        \n",
    "        return target, window\n",
    "    \n",
    "    def get_enhanced_relatedness(self, target_pos, target_texts, window_texts):\n",
    "        relatedness = 0\n",
    "        for rel_pair in self.rel_pairs[self.rel_pairs_set][target_pos]:\n",
    "            relatedness = relatedness + self.get_overlap_score(target_texts[rel_pair[0]], window_texts[rel_pair[1]])\n",
    "        return relatedness\n",
    "    \n",
    "    \n",
    "    # mass =  wn.synsets(\"mass\", wn.NOUN)[0]\n",
    "    # print(mass)\n",
    "    # kilogram = wn.synsets(\"kilogram\", wn.NOUN)[0]\n",
    "    # print(kilogram)\n",
    "    # target_tuple, window_tuple =(mass, 'NOUN'), (kilogram, 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # relatedness_score = disambiguator.get_relatedness(target_tuple, window_tuple)\n",
    "    # print(relatedness_score)\n",
    "    def get_relatedness(self, target_tuple, window_tuple):\n",
    "        target_texts, window_texts = self.get_texts(target_tuple, window_tuple)\n",
    "        if target_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "            relatedness = self.get_enhanced_relatedness(target_tuple[1], target_texts, window_texts)\n",
    "        else:\n",
    "            relatedness = self.get_overlap_score(\"default\", target_texts, window_texts)\n",
    "            \n",
    "        return relatedness\n",
    "    \n",
    "    def process_gloss(self, gloss):\n",
    "        processed_text = nltk.word_tokenize(gloss)\n",
    "        processed_text = [ word for word in processed_text if word not in stopwords.words('english') ]\n",
    "        processed_text = [ word.lower() for word in processed_text if word.isalnum() ]\n",
    "        processed_text = [ self.stemmer.stem(word) for word in processed_text ]\n",
    "        processed_gloss = ' '.join(processed_text)\n",
    "        \n",
    "        return processed_gloss\n",
    "    \n",
    "    def get_gloss_for_sense(self, sense, process_gloss=True):\n",
    "        gloss_for_sense = \"\"\n",
    "        gloss_for_sense = gloss_for_sense + sense.definition()\n",
    "        for example in sense.examples():\n",
    "            gloss_for_sense =gloss_for_sense + \". \" + example\n",
    "        if process_gloss :    \n",
    "            gloss_for_sense = self.process_gloss(gloss_for_sense)\n",
    "        return gloss_for_sense\n",
    "    \n",
    "    def merge_glosses(self, synsets):\n",
    "        aggregator = \"\"\n",
    "        for synset in synsets:\n",
    "            aggregator = aggregator + \". \" + self.get_gloss_for_sense(synset)\n",
    "        return aggregator\n",
    "    \n",
    "    def get_hyponyms_for_sense(self, sense):\n",
    "        hyponyms_for_sense = self.merge_glosses(sense.hyponyms())\n",
    "        return hyponyms_for_sense\n",
    "    \n",
    "    def get_meronyms_for_sense(self, sense):\n",
    "        meronyms_for_sense = self.merge_glosses(sense.member_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.substance_meronyms())\n",
    "        meronyms_for_sense = meronyms_for_sense + \". \" + self.merge_glosses(sense.part_meronyms())\n",
    "        return meronyms_for_sense\n",
    "    \n",
    "    def get_antonyms_for_sense(self, sense):\n",
    "        antonyms_for_sense = \"\"\n",
    "        for lemma in sense.lemmas():\n",
    "            antonyms_for_sense = antonyms_for_sense + \". \" + self.merge_glosses([ant.synset() for ant in lemma.antonyms()])\n",
    "        return antonyms_for_sense\n",
    "    \n",
    "    def get_similarity_for_sense(self, sense):\n",
    "        similarity_for_sense = self.merge_glosses(sense.similar_tos())\n",
    "        return similarity_for_sense\n",
    "    \n",
    "    def get_entailments_for_sense(self, sense):\n",
    "        entailments_for_sense = self.merge_glosses(sense.entailments())\n",
    "        return entailments_for_sense\n",
    "    \n",
    "    def get_also_sees_for_sense(self, sense):\n",
    "        also_sees_for_sense = self.merge_glosses(sense.also_sees())\n",
    "        return also_sees_for_sense\n",
    "    \n",
    "    def get_causes_for_sense(self, sense):\n",
    "        causes_for_sense = self.merge_glosses(sense.causes())\n",
    "        return causes_for_sense\n",
    "    \n",
    "    def get_target_sense_score(self, target_sense_tuple):\n",
    "        target_sense_score = 0\n",
    "        \n",
    "        for window_word in self.window_words:\n",
    "            #print(window_word)\n",
    "            window_word_senses = self.get_tuple_senses(window_word)\n",
    "            for window_word_sense in window_word_senses:\n",
    "                relatedness = self.get_relatedness(target_sense_tuple, (window_word_sense, window_word[1]))\n",
    "                target_sense_score = target_sense_score + relatedness\n",
    "        \n",
    "        return target_sense_score\n",
    "    \n",
    "    def remove_stop_words(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text: \n",
    "            if word_tuple[0] not in stopwords.words('english'): \n",
    "                if word_tuple[1] in [\"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "                    filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def remove_punctuation(self, processed_text):\n",
    "        filtered_text = []\n",
    "        for word_tuple in processed_text:  \n",
    "            if (len(word_tuple) == 2):\n",
    "                if (word_tuple[1] != \".\") : # word_tuple[0] not in [\".\", \"`\"]: \n",
    "                    filtered_text.append(word_tuple)\n",
    "        return filtered_text\n",
    "    \n",
    "    def get_target_position(self, processed_text, stemmed_target):\n",
    "        for i in range(0,len(processed_text)):\n",
    "            if stemmed_target == processed_text[i][0]:\n",
    "                return processed_text[i], i\n",
    "        return None, -1\n",
    "    \n",
    "    # word_tuple = ('mass', 'NOUN')\n",
    "    # disambiguator = Disambiguator()\n",
    "    # tuple_senses = disambiguator.get_tuple_senses(word_tuple)\n",
    "    # print(tuple_senses)\n",
    "    # # [Synset('mass.n.01'), Synset('batch.n.02'), Synset('mass.n.03'), Synset('mass.n.04'), Synset('mass.n.05'), Synset('multitude.n.03'), Synset('bulk.n.02'), Synset('mass.n.08'), Synset('mass.n.09')]\n",
    "    def get_tuple_senses(self, word_tuple):\n",
    "        return wn.synsets(word_tuple[0], self.porter_to_wn[word_tuple[1]])\n",
    "    \n",
    "    def num_words_on_the_right(self, target_position, processed_text):\n",
    "        return (len(processed_text) - target_position - 1)\n",
    "    \n",
    "    # processed_text = [('the', 'DET'), ('mass', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('object', 'NOUN'), ('is', 'VERB'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    # stemmed_target = \"mass\"\n",
    "    # disambiguator = Disambiguator()\n",
    "    # target_tuple, window_words = disambiguator.get_window_words(processed_text, stemmed_target)\n",
    "    # print(target_tuple)\n",
    "    # print(window_words)\n",
    "    # # ('mass', 'NOUN')\n",
    "    # # [('object', 'NOUN'), ('ten', 'ADJ'), ('kilogram', 'NOUN')]\n",
    "    def get_window_words(self, processed_text, stemmed_target):\n",
    "        processed_text = self.remove_punctuation(processed_text)\n",
    "        processed_text = self.remove_stop_words(processed_text)\n",
    "        target_tuple, target_position = self.get_target_position(processed_text, stemmed_target)\n",
    "        if target_position == -1:\n",
    "            print(\"Target word is not in the text!\")\n",
    "            return None, -1\n",
    "        if (self.window_size * 2 + 1) > len(processed_text):\n",
    "            window_words = processed_text[0:target_position] + processed_text[target_position+1:]\n",
    "        elif 0 < (self.window_size - target_position): \n",
    "            left_words = processed_text[0:target_position]\n",
    "            right_words = processed_text[target_position+1:2 * self.window_size + 1]\n",
    "            # target_position + self.window_size + 1 + (self.window_size - target_position) =  2 * self.window_size + 1\n",
    "            # print(\"Extract everything from the left and (self.window_size - target_position) more from the right\")\n",
    "            window_words=left_words+right_words\n",
    "        elif 0 < (self.window_size - self.num_words_on_the_right(target_position, processed_text) ):\n",
    "            right_words = processed_text[target_position+1:]\n",
    "            left_words = processed_text[len(processed_text) - 2*self.window_size - 1:target_position]\n",
    "            # target_position-self.window_size-(self.window_size - (len(processed_text) - target_position - 1)) =\n",
    "            # = -2*self.window_size + target_position - target_position + len(processed_text) - 1 = \n",
    "            # = len(processed_text) - 2*self.window_size - 1\n",
    "            # print(\"Extract everything from the right and (self.window_size - self.num_words_on_the_right) more from the left\")\n",
    "            window_words=left_words+right_words\n",
    "        else:\n",
    "            left_words = processed_text[target_position-self.window_size:target_position]\n",
    "            right_words = processed_text[target_position+1:target_position+1+self.window_size]\n",
    "            # print(\"Extract self.window_size word tuples from each side\")\n",
    "            window_words=left_words+right_words\n",
    "        return target_tuple, window_words\n",
    "    \n",
    "    def get_stemmed_pairs(self, word_pos_pairs):\n",
    "        stemmed_pairs = []\n",
    "        for word_pos_pair in word_pos_pairs:\n",
    "            stemmed_pair = (self.stemmer.stem(word_pos_pair[0]), word_pos_pair[1])\n",
    "            stemmed_pairs.append(stemmed_pair)\n",
    "            \n",
    "        return stemmed_pairs\n",
    "    \n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.get_processed(\"the mass of the object is ten kilograms\")\n",
    "    def get_processed(self, text_string):\n",
    "        tokenized_words = nltk.word_tokenize(text_string)\n",
    "#         print(\"Tokens with punctuation\")\n",
    "#         print(tokenized_words_with_punctuation)\n",
    "#         tokenized_words = self.remove_punctuation(tokenized_words_with_punctuation)\n",
    "#         print(\"Tokens\")\n",
    "#         print(tokenized_words)\n",
    "        word_pos_pairs = nltk.pos_tag(tokenized_words, tagset='universal')\n",
    "#         print(\"POS\")\n",
    "#         print(word_pos_pairs)\n",
    "        stemmed_pairs = self.get_stemmed_pairs(word_pos_pairs)\n",
    "#         print(\"Stemmed\")\n",
    "#         print(stemmed_pairs)\n",
    "        \n",
    "        return stemmed_pairs\n",
    "        \n",
    "    # eg:\n",
    "    # disambiguator = Disambiguator()\n",
    "    # disambiguator.disambiguate(\"the mass of the object is ten kilograms\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - property of physical body)\n",
    "    # disambiguator.disambiguate(\"the angry mass of people went after him\", \"mass\")\n",
    "    # # The word \"mass\" has the meaning from synset *****( gloss of synset ***** - crowd)       \n",
    "    def disambiguate(self, text_string, target_word):\n",
    "        disambiguated_sense = \"Not Implemented Yet\"\n",
    "        processed_text = self.get_processed(text_string)\n",
    "#         print(processed_text)\n",
    "        stemmed_target = self.stemmer.stem(target_word)\n",
    "        \n",
    "#         print(target_word)\n",
    "#         print(stemmed_target)\n",
    "        target_tuple, self.window_words = self.get_window_words(processed_text, stemmed_target)\n",
    "        if target_tuple == None:\n",
    "            return\n",
    "#         print(target_tuple)\n",
    "        target_senses = self.get_tuple_senses((target_word, target_tuple[1]))\n",
    "        best_score = (None, -1)\n",
    "#         print(target_senses)\n",
    "        for target_sense in target_senses:\n",
    "            target_sense_score = self.get_target_sense_score((target_sense, target_tuple[1]))\n",
    "            if target_sense_score > best_score[1] :\n",
    "                best_score = (target_sense, target_sense_score)\n",
    "        \n",
    "        predicted_synset = best_score[0]\n",
    "#         disambiguated_sense = self.get_gloss_for_sense(best_score[0], process_gloss=False)\n",
    "        disambiguated_sense = predicted_synset.definition()\n",
    "        return predicted_synset, disambiguated_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DisambiguatorTester:\n",
    "    def __init__(self, window_size=3, rel_pairs_set=\"old\"):\n",
    "        self.disambiguator = Disambiguator(window_size, rel_pairs_set)\n",
    "        self.sense_map = {\n",
    "            \"HARD1\": [\"difficult.a.01\"],    # not easy, requiring great physical or mental\n",
    "            \"HARD2\": [\"hard.a.02\",          # dispassionate\n",
    "                      \"difficult.a.01\"],\n",
    "            \"HARD3\": [\"hard.a.03\"],         # resisting weight or pressure\n",
    "            \"interest_1\": [\"interest.n.01\"], # readiness to give attention\n",
    "            \"interest_2\": [\"interest.n.03\"], # quality of causing attention to be given to\n",
    "            \"interest_3\": [\"pastime.n.01\"],  # activity, etc. that one gives attention to\n",
    "            \"interest_4\": [\"sake.n.01\"],     # advantage, advancement or favor\n",
    "            \"interest_5\": [\"interest.n.05\"], # a share in a company or business\n",
    "            \"interest_6\": [\"interest.n.04\"], # money paid for the use of money\n",
    "            \"cord\": [\"line.n.18\"],          # something (as a cord or rope) that is long and thin and flexible\n",
    "            \"formation\": [\"line.n.01\",\"line.n.03\"], # a formation of people or things one beside another\n",
    "            \"text\": [\"line.n.05\"],                 # text consisting of a row of words written across a page or computer screen\n",
    "            \"phone\": [\"telephone_line.n.02\"],   # a telephone connection\n",
    "            \"product\": [\"line.n.22\"],       # a particular kind of product or merchandise\n",
    "            \"division\": [\"line.n.29\"],      # a conceptual separation or distinction\n",
    "            \"SERVE12\": [\"serve.v.02\"],       # do duty or hold offices; serve in a specific function\n",
    "            \"SERVE10\": [\"serve.v.06\"], # provide (usually but not necessarily food)\n",
    "            \"SERVE2\": [\"serve.v.01\"],       # serve a purpose, role, or function\n",
    "            \"SERVE6\": [\"service.v.01\"]      # be used by; as of a utility\n",
    "        }\n",
    "        \n",
    "    def get_test_case(self, instance):\n",
    "        pos = instance.position\n",
    "        target_word = ' '.join(w for (w,t) in instance.context[pos:pos+1])\n",
    "        \n",
    "#         print(instance.context[0:pos])\n",
    "        left = ' '.join( \"\" if word_tuple=='FRASL' else word_tuple[0] for word_tuple in instance.context[0:pos]) # .items()\n",
    "        right = ' '.join(w for (w,t) in instance.context[pos+1:])\n",
    "        phrase = left + \" \" + target_word + \" \" + right\n",
    "        \n",
    "        target_synsets_names = self.sense_map[instance.senses[0]]\n",
    "        target_synsets = [wn.synset(name) for name in target_synsets_names]\n",
    "        \n",
    "        return phrase, target_word, target_synsets\n",
    "        \n",
    "    def test(self, no_inst=1, start_inst=0):\n",
    "        correct_predictions, total_tests = 0, 0\n",
    "        corpus_correct_predictions, corpus_total_tests = {}, {}\n",
    "        corpuses = senseval.fileids()\n",
    "        for corpus in corpuses: #[2:]:\n",
    "            corpus_correct_predictions[corpus] = 0\n",
    "            corpus_total_tests[corpus] = 0\n",
    "            print(\"=\" * 100)\n",
    "            no_instances = len(senseval.instances(corpus))\n",
    "            print(\"Testing \" + corpus + \" with \" + str(no_instances) + \" instances.\" )\n",
    "            print(\"Testing \" + corpus)\n",
    "            print(\"=\" * 100)\n",
    "            for instance in senseval.instances(corpus)[start_inst:start_inst+no_inst]:\n",
    "                phrase, target_word, target_synsets = self.get_test_case(instance)\n",
    "                print(\"-\" * 100)\n",
    "                print(\"Phrase: \" + phrase)\n",
    "                print(\"_\" * 5)\n",
    "                print(\"Target Word: \" + target_word)\n",
    "                print(\"_\" * 5)\n",
    "                print(\"Target Synsets: \" + str(target_synsets) + \" meaning: \")\n",
    "                print(target_synsets[0].definition())\n",
    "                print(\"_\" * 5)\n",
    "                predicted_synset, description = self.disambiguator.disambiguate(phrase, target_word)\n",
    "                print(\"Predicted Synset: \" + str(predicted_synset) + \" meaning: \")\n",
    "                print(description)\n",
    "                if predicted_synset in target_synsets:\n",
    "                    correct_predictions += 1\n",
    "                    corpus_correct_predictions[corpus] += 1\n",
    "                total_tests +=1\n",
    "                corpus_total_tests[corpus] += 1\n",
    "        print(\"=\" * 100)\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Run \" + str(total_tests) + \" tests\")\n",
    "        print(\"Predicted \" + str(correct_predictions) + \" predictions\")\n",
    "        print(\"Percentage \" + str(correct_predictions / total_tests * 100) + \"%\")\n",
    "        for corpus in corpuses:\n",
    "            print(\"_\" * 5)\n",
    "            print(corpus + \" \" + str(corpus_correct_predictions[corpus]) \n",
    "                  + \" / \" + str(corpus_total_tests[corpus]) )\n",
    "            print(\"Percentage: \" + str(corpus_correct_predictions[corpus] / corpus_total_tests[corpus] * 100) + \"%\")\n",
    "        print(\"=\" * 100)\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tester = DisambiguatorTester(7, \"new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Testing hard.pos with 4333 instances.\n",
      "Testing hard.pos\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: even though many of the university 's 30 , 638 students are more conservative and career-oriented than their '60s and '70s predecessors , and it 's harder to drum up big crowds for a protest than it used to be , telegraph retains a mystical -- if mangy -- aura .\n",
      "_____\n",
      "Target Word: harder\n",
      "_____\n",
      "Target Synsets: [Synset('difficult.a.01')] meaning: \n",
      "not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
      "_____\n",
      "Predicted Synset: Synset('arduous.s.01') meaning: \n",
      "characterized by effort to the point of exhaustion; especially physical effort\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: it 's hard to put into words , but since that day i 've felt much more confident .\n",
      "_____\n",
      "Target Word: hard\n",
      "_____\n",
      "Target Synsets: [Synset('difficult.a.01')] meaning: \n",
      "not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
      "_____\n",
      "Predicted Synset: Synset('arduous.s.01') meaning: \n",
      "characterized by effort to the point of exhaustion; especially physical effort\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: it 's hard to believe i 'm seeing it start to rise out of the ground , \" sheriff harvey nyland said after a groundbreaking ceremony .\n",
      "_____\n",
      "Target Word: hard\n",
      "_____\n",
      "Target Synsets: [Synset('difficult.a.01')] meaning: \n",
      "not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
      "_____\n",
      "Predicted Synset: Synset('arduous.s.01') meaning: \n",
      "characterized by effort to the point of exhaustion; especially physical effort\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: `` and we are pleased to announce that already coke , pepsi and others have expressed enthusiasm and interest in participating in this expanded mtv service , further supporting their already established use of mtv as a vehicle to attract the hard to-reach 12-34-year-old demographic . ''\n",
      "_____\n",
      "Target Word: hard\n",
      "_____\n",
      "Target Synsets: [Synset('difficult.a.01')] meaning: \n",
      "not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
      "_____\n",
      "Predicted Synset: Synset('arduous.s.01') meaning: \n",
      "characterized by effort to the point of exhaustion; especially physical effort\n",
      "====================================================================================================\n",
      "Testing interest.pos with 2368 instances.\n",
      "Testing interest.pos\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: the purchases show the strong interest of japanese investors in u . s . mortgage-based instruments , fannie mae 's chairman , david o . maxwell , said at a news conference .\n",
      "_____\n",
      "Target Word: interest\n",
      "_____\n",
      "Target Synsets: [Synset('interest.n.01')] meaning: \n",
      "a sense of concern with and curiosity about someone or something\n",
      "_____\n",
      "Predicted Synset: Synset('interest.n.05') meaning: \n",
      "(law) a right or legal share of something; a financial involvement with something\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: first , they are designed to eliminate the risk of prepayment -- mortgage-backed securities can be retired early if interest rates decline , and such prepayment forces investors to redeploy their money at lower rates .\n",
      "_____\n",
      "Target Word: interest\n",
      "_____\n",
      "Target Synsets: [Synset('interest.n.04')] meaning: \n",
      "a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
      "_____\n",
      "Predicted Synset: Synset('interest.n.05') meaning: \n",
      "(law) a right or legal share of something; a financial involvement with something\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: at the same time , the drop in interest rates since the spring has failed to revive the residential construction industry .\n",
      "_____\n",
      "Target Word: interest\n",
      "_____\n",
      "Target Synsets: [Synset('interest.n.04')] meaning: \n",
      "a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
      "_____\n",
      "Predicted Synset: Synset('interest.n.05') meaning: \n",
      "(law) a right or legal share of something; a financial involvement with something\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: david berson , economist for the mortgage bankers association , predicted the drop in interest rates eventually will boost spending on single-family homes , but probably not until early next year .\n",
      "_____\n",
      "Target Word: interest\n",
      "_____\n",
      "Target Synsets: [Synset('interest.n.04')] meaning: \n",
      "a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
      "_____\n",
      "Predicted Synset: Synset('interest.n.05') meaning: \n",
      "(law) a right or legal share of something; a financial involvement with something\n",
      "====================================================================================================\n",
      "Testing line.pos with 4146 instances.\n",
      "Testing line.pos\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: he introduced his children to the sport as toddlers , and wasn 't about to take no for an answer . when camille balked at taking the line at age four , she was left to ponder her choice in the middle of a lake .\n",
      "_____\n",
      "Target Word: line\n",
      "_____\n",
      "Target Synsets: [Synset('line.n.18')] meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "_____\n",
      "Predicted Synset: Synset('line.n.11') meaning: \n",
      "a spatial location defined by a real or imaginary unidimensional extent\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: no one and nothing seems prettified . when the widow elma radnor ( mary mcdonnell ) matter-of-factly fills joe in on the routine mine tragedies at matewan as she hangs clothes on the line , her bone-weariness is palpable .\n",
      "_____\n",
      "Target Word: line\n",
      "_____\n",
      "Target Synsets: [Synset('line.n.18')] meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "_____\n",
      "Predicted Synset: Synset('line.n.18') meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: it is known as an aggressive , tenacious litigator . richard d . parsons , a partner at patterson , belknap , webb & tyler , likens the experience of opposing sullivan & cromwell to \" having a thousand-pound tuna on the line . \"\n",
      "_____\n",
      "Target Word: line\n",
      "_____\n",
      "Target Synsets: [Synset('line.n.18')] meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "_____\n",
      "Predicted Synset: Synset('line.n.03') meaning: \n",
      "a formation of people or things one behind another\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: tipped off that mr . willaby is \" noodling \" -- catching fish illegally by snagging them in the gills or flesh -- two wildlife officials follow the 250-pound fisherman out to cedar creek lake here . they watch him dive under water with a fishing pole and a very short line and catch two flathead catfish , one 25 pounds and the other 30 pounds .\n",
      "_____\n",
      "Target Word: line\n",
      "_____\n",
      "Target Synsets: [Synset('line.n.18')] meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "_____\n",
      "Predicted Synset: Synset('line.n.18') meaning: \n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "====================================================================================================\n",
      "Testing serve.pos with 4378 instances.\n",
      "Testing serve.pos\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: above us through the window in the roof , the neighbor children peeked through and giggled as they watched the strangers who had come to their village . then our meal was served , and it was obvious there had been great preparation for us .\n",
      "_____\n",
      "Target Word: served\n",
      "_____\n",
      "Target Synsets: [Synset('serve.v.06')] meaning: \n",
      "provide (usually but not necessarily food)\n",
      "_____\n",
      "Predicted Synset: Synset('serve.v.10') meaning: \n",
      "work for or be a servant to\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: would everyone be willing to get up for 6 a . m . mass on sunday so jack could accept a nine-o 'clock golf invitation ? jack 's sons helped clear the table , and his wife served coffee and dessert .\n",
      "_____\n",
      "Target Word: served\n",
      "_____\n",
      "Target Synsets: [Synset('serve.v.06')] meaning: \n",
      "provide (usually but not necessarily food)\n",
      "_____\n",
      "Predicted Synset: Synset('serve.v.05') meaning: \n",
      "help to some food; help with food or drink\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: pound cake was a favorite throughout the south . it was served at both mount vernon and williamsburg .\n",
      "_____\n",
      "Target Word: served\n",
      "_____\n",
      "Target Synsets: [Synset('serve.v.06')] meaning: \n",
      "provide (usually but not necessarily food)\n",
      "_____\n",
      "Predicted Synset: Synset('suffice.v.01') meaning: \n",
      "be sufficient; be adequate, either in quality or quantity\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Phrase: bake in a dg325 oven for 35 minutes . serve with melted butter .\n",
      "_____\n",
      "Target Word: serve\n",
      "_____\n",
      "Target Synsets: [Synset('serve.v.06')] meaning: \n",
      "provide (usually but not necessarily food)\n",
      "_____\n",
      "Predicted Synset: Synset('serve.n.01') meaning: \n",
      "(sports) a stroke that puts the ball in play\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Run 16 tests\n",
      "Predicted 2 predictions\n",
      "Percentage 12.5%\n",
      "_____\n",
      "hard.pos 0 / 4\n",
      "Percentage: 0.0%\n",
      "_____\n",
      "interest.pos 0 / 4\n",
      "Percentage: 0.0%\n",
      "_____\n",
      "line.pos 2 / 4\n",
      "Percentage: 50.0%\n",
      "_____\n",
      "serve.pos 0 / 4\n",
      "Percentage: 0.0%\n",
      "====================================================================================================\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "tester.test(4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis = Disambiguator(7, \"new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Synset('nurse.n.01'),\n",
       " 'one skilled in caring for young children or the sick (usually under the supervision of a physician)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.disambiguate(\"the hospital hired a new nurse for the pacients\",\"nurse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Synset('doctor.n.01'), 'a licensed medical practitioner')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.disambiguate(\"the hospital hired a new doctor for the pacients\",\"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Synset('doctor.n.01'), 'a licensed medical practitioner')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.disambiguate(\"the doctor attended the conference at the university\",\"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
